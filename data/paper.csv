paper_id,title,year,abstract,doi,venue,fieldsOfStudy,keywords,venue_type,pages
f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,2017.0,"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",,arXiv.org,"Computer Science,Mathematics",machine learning,workshop,13
4954fa180728932959997a4768411ff9136aac81,TensorFlow: A system for large-scale machine learning,2016.0,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",,USENIX Symposium on Operating Systems Design and Implementation,Computer Science,machine learning,conference,6
9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,2016.0,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",,arXiv.org,Computer Science,machine learning,workshop,15
bc00ff34ec7772080c7039b17f7069a2f7df0889,Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,2018.0,,10.1038/s42256-019-0048-x,Nature Machine Intelligence,"Medicine,Computer Science",machine learning,journal,11
0090023afc66cd2741568599057f4e82b566137c,A Survey on Bias and Fairness in Machine Learning,2019.0,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",10.1145/3457607,ACM Computing Surveys,"Computer Science,Mathematics",machine learning,journal,5
794b3ffd28d28606230efc975eeec9f0522fb139,An Introduction to Machine Learning,2017.0,,10.1007/978-3-319-63913-0,Cambridge International Law Journal,Computer Science,machine learning,journal,6
f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,2015.0,"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",,Neural Information Processing Systems,Computer Science,machine learning,workshop,13
53c9f3c34d8481adaf24df3b25581ccf1bc53f5c,Physics-informed machine learning,2021.0,,10.1038/s42254-021-00314-5,Nature Reviews Physics,,machine learning,workshop,14
8e3c872076750bcce868808f9d4d7a038f950040,Pattern Recognition And Machine Learning,2016.0,,,,Computer Science,machine learning,workshop,8
597bd2e45427563cdf025e53a3239006aa364cfc,Open Graph Benchmark: Datasets for Machine Learning on Graphs,2020.0,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .",,Neural Information Processing Systems,"Computer Science,Mathematics",machine learning,workshop,7
f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d,Membership Inference Attacks Against Machine Learning Models,2016.0,"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",10.1109/SP.2017.41,IEEE Symposium on Security and Privacy,"Computer Science,Mathematics",machine learning,conference,13
9f5b82d9915d0752957602224c5056be7e749c83,Foundations of Machine Learning,2021.0,,10.2139/ssrn.3399990,Introduction to AI Techniques for Renewable Energy Systems,,machine learning,workshop,14
7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea,"Machine Learning: Algorithms, Real-World Applications and Research Directions",2021.0,"In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers in various real-world situations and application areas, particularly from the technical point of view.",10.1007/s42979-021-00592-x,SN Computer Science,"Computer Science,Medicine",machine learning,workshop,10
ad4fd2c149f220a62441576af92a8a669fe81246,Scikit-learn: Machine Learning in Python,2011.0,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",10.5555/1953048.2078195,Journal of machine learning research,Computer Science,machine learning,journal,12
d422df8bff4e677a3077635db116679d25142bfc,"Machine learning: Trends, perspectives, and prospects",2015.0,"Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.",10.1126/science.aaa8415,Science,Medicine,machine learning,workshop,9
5c39e37022661f81f79e481240ed9b175dec6513,Towards A Rigorous Science of Interpretable Machine Learning,2017.0,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",,,"Computer Science,Mathematics",machine learning,workshop,7
807c1f19047f96083e13614f7ce20f2ac98c239a,C4.5: Programs for Machine Learning,1992.0,,,,Computer Science,machine learning,workshop,14
730ca170962a58607e092035beb2afc4b5fa6242,Data Mining Practical Machine Learning Tools and Techniques,2014.0,,,,Computer Science,machine learning,workshop,5
360ca02e6f5a5e1af3dce4866a257aafc2d6d6f5,Machine learning - a probabilistic perspective,2012.0,,,Adaptive computation and machine learning series,Computer Science,machine learning,workshop,5
56e8863838b4dcc4790108cd1e7e680a104a7c30,Machine Learning Algorithms: A Review,2022.0,.,10.21275/sr22815163219,International Journal of Science and Research (IJSR),Computer Science,machine learning,journal,8
d8384f7ef288d2d5cb267128471c5427fc98b54b,An Introduction to Variable and Feature Selection,2003.0,,,Journal of machine learning research,Computer Science,feature selection,journal,12
ba969a4f3bc5f5a84f6025478e566c40661d85f3,"Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",2003.0,"Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.",10.1109/TPAMI.2005.159,IEEE Transactions on Pattern Analysis and Machine Intelligence,"Computer Science,Medicine",feature selection,journal,14
7c54d9b80113f637e447199d8ffbfe4b0678cc6f,A survey on feature selection methods,2014.0,,10.1016/j.compeleceng.2013.11.024,Computers & electrical engineering,Computer Science,feature selection,workshop,13
c3ebcef26c22a373b6f26a67934213eb0582804e,A Comparative Study on Feature Selection in Text Categorization,1997.0,,,International Conference on Machine Learning,Computer Science,feature selection,conference,10
6bc43977fb11cceed0b9aa55b23c6dd29dd9a132,Correlation-based Feature Selection for Machine Learning,2003.0,,,,Computer Science,feature selection,workshop,7
66c98f818ed342832b27ddac69998e8ea63fc7bf,Adapting Feature Selection Algorithms for the Classification of Chinese Texts,2023.0,"Text classification has been highlighted as the key process to organize online texts for better communication in the Digital Media Age. Text classification establishes classification rules based on text features, so the accuracy of feature selection is the basis of text classification. Facing fast-increasing Chinese electronic documents in the digital environment, scholars have accumulated quite a few algorithms for the feature selection for the automatic classification of Chinese texts in recent years. However, discussion about how to adapt existing feature selection algorithms for various types of Chinese texts is still inadequate. To address this, this study proposes three improved feature selection algorithms and tests their performance on different types of Chinese texts. These include an enhanced CHI square with mutual information (MI) algorithm, which simultaneously introduces word frequency and term adjustment (CHMI); a term frequency–CHI square (TF–CHI) algorithm, which enhances weight calculation; and a term frequency–inverse document frequency (TF–IDF) algorithm enhanced with the extreme gradient boosting (XGBoost) algorithm, which improves the algorithm’s ability of word filtering (TF–XGBoost). This study randomly chooses 3000 texts from six different categories of the Sogou news corpus to obtain the confusion matrix and evaluate the performance of the new algorithms with precision and the F1-score. Experimental comparisons are conducted on support vector machine (SVM) and naive Bayes (NB) classifiers. The experimental results demonstrate that the feature selection algorithms proposed in this paper improve performance across various news corpora, although the best feature selection schemes for each type of corpus are different. Further studies of the application of the improved feature selection methods in other languages and the improvement in classifiers are suggested.",10.3390/systems11090483,Syst.,Computer Science,feature selection,workshop,12
42c5e4459560852f3b222f0c9fd2838a1e14a4c3,Unsupervised feature selection via multiple graph fusion and feature weight learning,2023.0,,10.1007/s11432-022-3579-1,Science China Information Sciences,Computer Science,feature selection,workshop,7
ecc2ca3150dc4d4d8dceedab244114f191e05742,Feature Selection with the Boruta Package,2010.0,"This article describes a R package Boruta, implementing a novel feature selection algorithm for finding emph{all relevant variables}. The algorithm is designed as a wrapper around a Random Forest classification algorithm. It iteratively removes the features which are proved by a statistical test to be less relevant than random probes. The Boruta package provides a convenient interface to the algorithm. The short description of the algorithm and examples of its application are presented.",10.18637/JSS.V036.I11,,Computer Science,feature selection,workshop,14
13653c06f1cecd84cc692f6195c33cc9b849139c,mixOmics: An R package for ‘omics feature selection and multiple data integration,2017.0,"The advent of high throughput technologies has led to a wealth of publicly available ‘omics data coming from different sources, such as transcriptomics, proteomics, metabolomics. Combining such large-scale biological data sets can lead to the discovery of important biological insights, provided that relevant information can be extracted in a holistic manner. Current statistical approaches have been focusing on identifying small subsets of molecules (a ‘molecular signature’) to explain or predict biological conditions, but mainly for a single type of ‘omics. In addition, commonly used methods are univariate and consider each biological feature independently. We introduce mixOmics, an R package dedicated to the multivariate analysis of biological data sets with a specific focus on data exploration, dimension reduction and visualisation. By adopting a system biology approach, the toolkit provides a wide range of methods that statistically integrate several data sets at once to probe relationships between heterogeneous ‘omics data sets. Our recent methods extend Projection to Latent Structure (PLS) models for discriminant analysis, for data integration across multiple ‘omics data or across independent studies, and for the identification of molecular signatures. We illustrate our latest mixOmics integrative frameworks for the multivariate analyses of ‘omics data available from the package.",10.1371/journal.pcbi.1005752,bioRxiv,"Computer Science,Medicine,Biology",feature selection,workshop,14
911fbaec109f72130815e05e2633ec879590382c,A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction,2022.0,"Machine learning has shown utility in detecting patterns within large, unstructured, and complex datasets. One of the promising applications of machine learning is in precision medicine, where disease risk is predicted using patient genetic data. However, creating an accurate prediction model based on genotype data remains challenging due to the so-called “curse of dimensionality” (i.e., extensively larger number of features compared to the number of samples). Therefore, the generalizability of machine learning models benefits from feature selection, which aims to extract only the most “informative” features and remove noisy “non-informative,” irrelevant and redundant features. In this article, we provide a general overview of the different feature selection methods, their advantages, disadvantages, and use cases, focusing on the detection of relevant features (i.e., SNPs) for disease risk prediction.",10.3389/fbinf.2022.927312,Frontiers in Bioinformatics,"Computer Science,Medicine",feature selection,workshop,9
98c25683fc8d6446448b734b1bcf08e1457f8d85,A review of feature selection techniques in bioinformatics,2007.0,"Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.",10.1093/bioinformatics/btm344,Bioinform.,"Computer Science,Medicine",feature selection,workshop,13
f0d4ddf7167d03d0992844bc29f4c1586f37841e,Metaheuristic Algorithms on Feature Selection: A Survey of One Decade of Research (2009-2019),2021.0,"Feature selection is a critical and prominent task in machine learning. To reduce the dimension of the feature set while maintaining the accuracy of the performance is the main aim of the feature selection problem. Various methods have been developed to classify the datasets. However, metaheuristic algorithms have achieved great attention in solving numerous optimization problem. Therefore, this paper presents an extensive literature review on solving feature selection problem using metaheuristic algorithms which are developed in the ten years (2009-2019). Further, metaheuristic algorithms have been classified into four categories based on their behaviour. Moreover, a categorical list of more than a hundred metaheuristic algorithms is presented. To solve the feature selection problem, only binary variants of metaheuristic algorithms have been reviewed and corresponding to their categories, a detailed description of them explained. The metaheuristic algorithms in solving feature selection problem are given with their binary classification, name of the classifier used, datasets and the evaluation metrics. After reviewing the papers, challenges and issues are also identified in obtaining the best feature subset using different metaheuristic algorithms. Finally, some research gaps are also highlighted for the researchers who want to pursue their research in developing or modifying metaheuristic algorithms for classification. For an application, a case study is presented in which datasets are adopted from the UCI repository and numerous metaheuristic algorithms are employed to obtain the optimal feature subset.",10.1109/ACCESS.2021.3056407,IEEE Access,Computer Science,feature selection,journal,5
7da309bf7cd04a2cc5b4047e40c0caae996a01f6,A comprehensive survey on recent metaheuristics for feature selection,2022.0,,10.1016/j.neucom.2022.04.083,Neurocomputing,Computer Science,feature selection,workshop,13
05768e8a692d7122f1c54e8bc3a87ef36df235a4,A Comprehensive Review of Dimensionality Reduction Techniques for Feature Selection and Feature Extraction,2020.0,"Due to sharp increases in data dimensions, working on every data mining or machine learning (ML) task requires more efficient techniques to get the desired results. Therefore, in recent years, researchers have proposed and developed many methods and techniques to reduce the high dimensions of data and to attain the required accuracy. To ameliorate the accuracy of learning features as well as to decrease the training time dimensionality reduction is used as a pre-processing step, which can eliminate irrelevant data, noise, and redundant features. Dimensionality reduction (DR) has been performed based on two main methods, which are feature selection (FS) and feature extraction (FE). FS is considered an important method because data is generated continuously at an ever-increasing rate; some serious dimensionality problems can be reduced with this method, such as decreasing redundancy effectively, eliminating irrelevant data, and ameliorating result comprehensibility. Moreover, FE transacts with the problem of finding the most distinctive, informative, and decreased set of features to ameliorate the efficiency of both the processing and storage of data. This paper offers a comprehensive approach to FS and FE in the scope of DR. Moreover, the details of each paper, such as used algorithms/approaches, datasets, classifiers, and achieved results are comprehensively analyzed and summarized. Besides, a systematic discussion of all of the reviewed methods to highlight authors' trends, determining the method(s) has been done, which significantly reduced computational time, and selecting the most accurate classifiers. As a result, the different types of both methods have been discussed and analyzed the findings.  ",10.38094/jastt1224,Journal of Applied Science and Technology Trends,Computer Science,feature selection,journal,7
17d7be6cb7312ed6942d829e674eb84abe717524,A comprehensive survey on feature selection in the various fields of machine learning,2021.0,,10.1007/s10489-021-02550-9,Applied intelligence (Boston),Computer Science,feature selection,workshop,5
095c69574a9b079c5d9ce9ae4b0c196d6e1e4d60,Multiclass feature selection with metaheuristic optimization algorithms: a review,2022.0,"Selecting relevant feature subsets is vital in machine learning, and multiclass feature selection is harder to perform since most classifications are binary. The feature selection problem aims at reducing the feature set dimension while maintaining the performance model accuracy. Datasets can be classified using various methods. Nevertheless, metaheuristic algorithms attract substantial attention to solving different problems in optimization. For this reason, this paper presents a systematic survey of literature for solving multiclass feature selection problems utilizing metaheuristic algorithms that can assist classifiers selects optima or near optima features faster and more accurately. Metaheuristic algorithms have also been presented in four primary behavior-based categories, i.e., evolutionary-based, swarm-intelligence-based, physics-based, and human-based, even though some literature works presented more categorization. Further, lists of metaheuristic algorithms were introduced in the categories mentioned. In finding the solution to issues related to multiclass feature selection, only articles on metaheuristic algorithms used for multiclass feature selection problems from the year 2000 to 2022 were reviewed about their different categories and detailed descriptions. We considered some application areas for some of the metaheuristic algorithms applied for multiclass feature selection with their variations. Popular multiclass classifiers for feature selection were also examined. Moreover, we also presented the challenges of metaheuristic algorithms for feature selection, and we identified gaps for further research studies.",10.1007/s00521-022-07705-4,Neural computing & applications (Print),"Computer Science,Medicine",feature selection,workshop,15
dd2c89052f4dfb1f1d060b08374469aa1e0f57a6,(AF)2-S3Net: Attentive Feature Fusion with Adaptive Feature Selection for Sparse Semantic Segmentation Network,2021.0,"Autonomous robotic systems and self driving cars rely on accurate perception of their surroundings as the safety of the passengers and pedestrians is the top priority. Semantic segmentation is one of the essential components of road scene perception that provides semantic information of the surrounding environment. Recently, several methods have been introduced for 3D LiDAR semantic segmentation. While they can lead to improved performance, they are either afflicted by high computational complexity, therefore are inefficient, or they lack fine details of smaller object instances. To alleviate these problems, we propose (AF)2-S3Net, an end-to-end encoder-decoder CNN network for 3D LiDAR semantic segmentation. We present a novel multibranch attentive feature fusion module in the encoder and a unique adaptive feature selection module with feature map re-weighting in the decoder. Our (AF)2-S3Net fuses the voxel-based learning and point-based learning methods into a unified framework to effectively process the potentially large 3D scene. Our experimental results show that the proposed method outperforms the state-of-the-art approaches on the large-scale nuScenes-lidarseg and SemanticKITTI benchmark, ranking 1st on both competitive public leaderboard competitions upon publication.",10.1109/CVPR46437.2021.01236,Computer Vision and Pattern Recognition,Computer Science,feature selection,workshop,5
6573fa74af238d1e3538c026997e31b9f67f19f7,"Shapley values for feature selection: The good, the bad, and the axioms",2021.0,"The Shapley value has become popular in the Explainable AI (XAI) literature, thanks, to a large extent, to a solid theoretical foundation, including four “favourable and fair” axioms for attribution in transferable utility games. The Shapley value is provably the only solution concept satisfying these axioms. In this paper, we introduce the Shapley value and draw attention to its recent uses as a feature selection tool. We call into question this use of the Shapley value, using simple, abstract “toy” counterexamples to illustrate that the axioms may work against the goals of feature selection. From this, we develop a number of insights that are then investigated in concrete simulation settings, with a variety of Shapley value formulations, including SHapley Additive exPlanations (SHAP) and Shapley Additive Global importancE (SAGE). The aim is not to encourage any use of the Shapley value for feature selection, but we aim to clarify various limitations around their current use in the literature. In so doing, we hope to help demystify certain aspects of the Shapley value axioms that are viewed as “favourable”. In particular, we wish to highlight that the favourability of the axioms depends non-trivially on the way in which the Shapley value is appropriated in the XAI application.",10.1109/ACCESS.2021.3119110,IEEE Access,"Computer Science,Mathematics",feature selection,journal,15
1739c6d0cb3bf9c52631520c9fde423179e95eab,Feature Selection Using Fuzzy Neighborhood Entropy-Based Uncertainty Measures for Fuzzy Neighborhood Multigranulation Rough Sets,2021.0,"For heterogeneous data sets containing numerical and symbolic feature values, feature selection based on fuzzy neighborhood multigranulation rough sets (FNMRS) is a very significant step to preprocess data and improve its classification performance. This article presents an FNMRS-based feature selection approach in neighborhood decision systems. First, some concepts of fuzzy neighborhood rough sets and neighborhood multigranulation rough sets are given, and then the FNMRS model is investigated to construct uncertainty measures. Second, the optimistic and pessimistic FNMRS models are built by using fuzzy neighborhood multigranulation lower and upper approximations from algebra view, and some fuzzy neighborhood entropy-based uncertainty measures are developed in information view. Inspired by both algebra and information views based on the FNMRS model, the fuzzy neighborhood pessimistic multigranulation entropy is proposed. Third, the Fisher score model is utilized to delete irrelevant features to decrease the complexity of high-dimensional data sets, and then, a forward feature selection algorithm is provided to promote the performance of heterogeneous data classification. Experimental results on 12 data sets show that the presented model is effective for selecting important features with the higher stability of classification in neighborhood decision systems.",10.1109/TFUZZ.2020.2989098,IEEE transactions on fuzzy systems,Computer Science,feature selection,journal,6
80915717a22b055846ca96de3731e5fe832f0413,Feature selection based on mutual information with correlation coefficient,2021.0,,10.1007/s10489-021-02524-x,Applied intelligence (Boston),Computer Science,feature selection,workshop,9
f134abeaf9bfd41f29b97aec675ec31895bf541d,High-performance medicine: the convergence of human and artificial intelligence,2019.0,,10.1038/s41591-018-0300-7,Nature Network Boston,"Computer Science,Medicine",artificial intelligence,journal,12
530a059cb48477ad1e3d4f8f4b153274c8997332,"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",2019.0,,10.1016/j.inffus.2019.12.012,Information Fusion,Computer Science,artificial intelligence,workshop,14
e89dfa306723e8ef031765e9c44e5f6f94fd8fda,Explanation in Artificial Intelligence: Insights from the Social Sciences,2017.0,,10.1016/J.ARTINT.2018.07.007,Artificial Intelligence,Computer Science,artificial intelligence,workshop,10
8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c,Sparks of Artificial General Intelligence: Early experiments with GPT-4,2023.0,"Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.",,arXiv.org,Computer Science,artificial intelligence,workshop,13
21dff47a4142445f83016da0819ffe6dd2947f66,Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),2018.0,"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.",10.1109/ACCESS.2018.2870052,IEEE Access,Computer Science,artificial intelligence,journal,11
4b4279db68b16e20fbc56f9d41980a950191d30a,"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence",1992.0,"From the Publisher:  Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.  In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.  Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements.  John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and isDirector of the University of Michigan/Santa Fe Institute Advanced Research Program.",10.7551/MITPRESS/1090.001.0001,,"Computer Science,Engineering",artificial intelligence,workshop,13
f92922a9fe4e6bb603291249796d80d09d1fd9f3,Impact of Artificial Intelligence in Customer Journey,2024.0,"The entire gamut of Customer journey is undergoing a massive transformation due to the rapid advancement of Artificial Intelligence (AI). Leveraging the power of AI , CRM & systems have refined the aspect of how businesses manage and optimize the customer journey. AI-powered systems have significant impact across various stages of the customer lifecycle by use of techniques such as machine learning to empower businesses to use systems that can analyse vast amounts of customer dataset in real-time, enabling them to gain deeper insights in customer behaviours, preferences, & sentiment. The AI-driven techniques help businesses to drive more personalized & targeted marketing campaigns, tailored recommendations, and extend efficient customer service leading ultimately to enhancing customer satisfaction and loyalty. Moreover, AI-powered systems have capabilities of offering predictive analytics which empower businesses to forecast customer behaviours and anticipate their needs. The capabilities help businesses in effective resource optimization and improve efficiency. For customer service AI-powered chatbots and virtual assistants are used to enhance engagement by providing instant responses and ability to handle resolving issues promptly.",10.38124/ijisrt/ijisrt24aug807,International Journal of Innovative Science and Research Technology,,artificial intelligence,journal,11
2633a948f06a02417a39c9ff4e9c948bbad460d7,Artificial Intelligence and the Future of Work,2024.0,,10.1007/978-3-030-69128-8_4,Reflections on Artificial Intelligence for Humanity,Computer Science,artificial intelligence,workshop,12
7b72711ac2ea7bd7f519cac162a4a6578bbb7d0d,ARTIFICIAL INTELLIGENCE FOR THE REAL WORLD,2023.0,,10.56726/irjmets42512,International Research Journal of Modernization in Engineering Technology and Science,,artificial intelligence,journal,15
e1d2f2a717aa03280126f87c8e5fad695f52bf7c,Explainable Artificial Intelligence (XAI),2023.0,"Explainable Artificial Intelligence (XAI) has emerged as a critical facet in the realm of machine learning and artificial intelligence, responding to the increasing complexity of models, particularly deep neural networks, and the subsequent need for transparent decision making processes. This research paper delves into the essence of XAI, unraveling its significance across diverse domains such as healthcare, finance, and criminal justice. As a countermeasure to the opacity of intricate models, the paper explores various XAI methods and techniques, including LIME and SHAP, weighing their interpretability against computational efficiency and accuracy. Through an examination of real-world applications, the research elucidates how XAI not only enhances decision-making processes but also influences user trust and acceptance in AI systems. However, the paper also scrutinizes the delicate balance between interpretability and performance, shedding light on instances where the pursuit of accuracy may compromise explain-ability. Additionally, it navigates through the current challenges and limitations in XAI, the regulatory landscape surrounding AI explain-ability, and offers insights into future trends and directions, fostering a comprehensive understanding of XAI's present state and future potential.",10.48047/ijfans/v12/i1/271,International Journal of Food and Nutritional Science,,artificial intelligence,journal,8
5cde474869cb230a29b3ba0f6f685f5162b1a1a1,Revolutionizing healthcare: the role of artificial intelligence in clinical practice,2023.0,"Introduction Healthcare systems are complex and challenging for all stakeholders, but artificial intelligence (AI) has transformed various fields, including healthcare, with the potential to improve patient care and quality of life. Rapid AI advancements can revolutionize healthcare by integrating it into clinical practice. Reporting AI’s role in clinical practice is crucial for successful implementation by equipping healthcare providers with essential knowledge and tools. Research Significance This review article provides a comprehensive and up-to-date overview of the current state of AI in clinical practice, including its potential applications in disease diagnosis, treatment recommendations, and patient engagement. It also discusses the associated challenges, covering ethical and legal considerations and the need for human expertise. By doing so, it enhances understanding of AI’s significance in healthcare and supports healthcare organizations in effectively adopting AI technologies. Materials and Methods The current investigation analyzed the use of AI in the healthcare system with a comprehensive review of relevant indexed literature, such as PubMed/Medline, Scopus, and EMBASE, with no time constraints but limited to articles published in English. The focused question explores the impact of applying AI in healthcare settings and the potential outcomes of this application. Results Integrating AI into healthcare holds excellent potential for improving disease diagnosis, treatment selection, and clinical laboratory testing. AI tools can leverage large datasets and identify patterns to surpass human performance in several healthcare aspects. AI offers increased accuracy, reduced costs, and time savings while minimizing human errors. It can revolutionize personalized medicine, optimize medication dosages, enhance population health management, establish guidelines, provide virtual health assistants, support mental health care, improve patient education, and influence patient-physician trust. Conclusion AI can be used to diagnose diseases, develop personalized treatment plans, and assist clinicians with decision-making. Rather than simply automating tasks, AI is about developing technologies that can enhance patient care across healthcare settings. However, challenges related to data privacy, bias, and the need for human expertise must be addressed for the responsible and effective implementation of AI in healthcare.",10.1186/s12909-023-04698-z,BMC Medical Education,Medicine,artificial intelligence,workshop,5
9faa2b0e5cb93f20df0555c3c350fab0b2eccf3a,Foundation models for generalist medical artificial intelligence,2023.0,"The exceptionally rapid development of highly flexible, reusable artificial intelligence (AI) models is likely to usher in newfound capabilities in medicine. We propose a new paradigm for medical AI, which we refer to as generalist medical AI (GMAI). GMAI models will be capable of carrying out a diverse set of tasks using very little or no task-specific labelled data. Built through self-supervision on large, diverse datasets, GMAI will flexibly interpret different combinations of medical modalities, including data from imaging, electronic health records, laboratory results, genomics, graphs or medical text. Models will in turn produce expressive outputs such as free-text explanations, spoken recommendations or image annotations that demonstrate advanced medical reasoning abilities. Here we identify a set of high-impact potential applications for GMAI and lay out specific technical capabilities and training datasets necessary to enable them. We expect that GMAI-enabled applications will challenge current strategies for regulating and validating AI devices for medicine and will shift practices associated with the collection of large medical datasets. This review discusses generalist medical artificial intelligence, identifying potential applications and setting out specific technical capabilities and training datasets necessary to enable them, as well as highlighting challenges to its implementation.",10.1038/s41586-023-05881-4,Nature,Medicine,artificial intelligence,journal,15
f08060425aa8a212d74185ee23a08329b89abcd2,Scientific discovery in the age of artificial intelligence,2023.0,,10.1038/s41586-023-06221-2,Nature,"Medicine,Computer Science",artificial intelligence,journal,14
e251ba9fe7992fc07a01365a5f8f2b4d9020b875,Artificial intelligence in higher education: the state of the field,2023.0,"This systematic review provides unique findings with an up-to-date examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using PRISMA principles and protocol, 138 articles were identified for a full examination. Using a priori, and grounded coding, the data from the 138 articles were extracted, analyzed, and coded. The findings of this study show that in 2021 and 2022, publications rose nearly two to three times the number of previous years. With this rapid rise in the number of AIEd HE publications, new trends have emerged. The findings show that research was conducted in six of the seven continents of the world. The trend has shifted from the US to China leading in the number of publications. Another new trend is in the researcher affiliation as prior studies showed a lack of researchers from departments of education. This has now changed to be the most dominant department. Undergraduate students were the most studied students at 72%. Similar to the findings of other studies, language learning was the most common subject domain. This included writing, reading, and vocabulary acquisition. In examination of who the AIEd was intended for 72% of the studies focused on students, 17% instructors, and 11% managers. In answering the overarching question of how AIEd was used in HE, grounded coding was used. Five usage codes emerged from the data: (1) Assessment/Evaluation, (2) Predicting, (3) AI Assistant, (4) Intelligent Tutoring System (ITS), and (5) Managing Student Learning. This systematic review revealed gaps in the literature to be used as a springboard for future researchers, including new tools, such as Chat GPT. A systematic review examining AIEd in higher education (HE) up to the end of 2022. Unique findings in the switch from US to China in the most studies published. A two to threefold increase in studies published in 2021 and 2022 to prior years. AIEd was used for: Assessment/Evaluation, Predicting, AI Assistant, Intelligent Tutoring System, and Managing Student Learning.",10.1186/s41239-023-00392-8,International Journal of Educational Technology in Higher Education,,artificial intelligence,journal,11
9dafa6c5c609348b46734fc8997b93b3587fec6e,Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education,2023.0,Generative artificial intelligence (AI) is ushering in an era of potential transformation of journalism and media content. This essay considers one notable generative AI platform called ChatGPT made available to the public in 2022 for free use. ChatGPT allows users to enter text prompts and rapidly generates text responses drawn from its knowledge acquired via machine learning in engagement with the internet. This essay is coauthored by a human journalism and media professor in collaboration with ChatGPT. The essay demonstrates the capacity and limitations of ChatGPT and offers reflections on the implications of generative AI for journalism and media education.,10.1177/10776958221149577,Journalism &amp; Mass Communication Educator,,artificial intelligence,journal,6
8d984ee2eeabb630014f31fc759d4980830c4bdb,Can artificial intelligence help for scientific writing?,2023.0,"This paper discusses the use of Artificial Intelligence Chatbot in scientific writing. ChatGPT is a type of chatbot, developed by OpenAI, that uses the Generative Pre-trained Transformer (GPT) language model to understand and respond to natural language inputs. AI chatbot and ChatGPT in particular appear to be useful tools in scientific writing, assisting researchers and scientists in organizing material, generating an initial draft and/or in proofreading. There is no publication in the field of critical care medicine prepared using this approach; however, this will be a possibility in the next future. ChatGPT work should not be used as a replacement for human judgment and the output should always be reviewed by experts before being used in any critical decision-making or application. Moreover, several ethical issues arise about using these tools, such as the risk of plagiarism and inaccuracies, as well as a potential imbalance in its accessibility between high- and low-income countries, if the software becomes paying. For this reason, a consensus on how to regulate the use of chatbots in scientific writing will soon be required.",10.1186/s13054-023-04380-2,Critical Care,Medicine,artificial intelligence,workshop,8
288078127a3078332230442170f6745ed333c700,"A Conversation on Artificial Intelligence, Chatbots, and Plagiarism in Higher Education",2023.0,,10.1007/s12195-022-00754-8,Cellular and Molecular Bioengineering,Medicine,artificial intelligence,workshop,11
8d020275181c69e5e768c6ffc40e09710a6f54f1,Experimental evidence on the productivity effects of generative artificial intelligence,2023.0,"We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment. Description Editor’s summary Automation has historically displaced human workers in factories (e.g., automotive manufacturing) or in performing routine computational tasks. Will generative artificial intelligence (AI) tools such as ChatGPT disrupt the labor market by making educated professionals obsolete, or will these tools complement their skills and enhance productivity? Noy and Zhang examined this issue in an experiment that recruited college-educated professionals to complete incentivized writing tasks. Participants assigned to use ChatGPT were more productive, efficient, and enjoyed the tasks more. Participants with weaker skills benefited the most from ChatGPT, which carries policy implications for efforts to reduce productivity inequality through AI. —EEU The assistive chatbot ChatGPT raises productivity in professional writing tasks and reduces productivity inequality.",10.1126/science.adh2586,Science,Medicine,artificial intelligence,workshop,9
6f4486c3d8ccd638c2a6bcbfafa01b5a3225bcab,Examining Science Education in ChatGPT: An Exploratory Study of Generative Artificial Intelligence,2023.0,"The advent of generative artificial intelligence (AI) offers transformative potential in the field of education. The study explores three main areas: (1) How did ChatGPT answer questions related to science education? (2) What are some ways educators could utilise ChatGPT in their science pedagogy? and (3) How has ChatGPT been utilised in this study, and what are my reflections about its use as a research tool? This exploratory research applies a self-study methodology to investigate the technology. Impressively, ChatGPT’s output often aligned with key themes in the research. However, as it currently stands, ChatGPT runs the risk of positioning itself as the ultimate epistemic authority, where a single truth is assumed without a proper grounding in evidence or presented with sufficient qualifications. Key ethical concerns associated with AI include its potential environmental impact, issues related to content moderation, and the risk of copyright infringement. It is important for educators to model responsible use of ChatGPT, prioritise critical thinking, and be clear about expectations. ChatGPT is likely to be a useful tool for educators designing science units, rubrics, and quizzes. Educators should critically evaluate any AI-generated resource and adapt it to their specific teaching contexts. ChatGPT was used as a research tool for assistance with editing and to experiment with making the research narrative clearer. The intention of the paper is to act as a catalyst for a broader conversation about the use of generative AI in science education.",10.1007/s10956-023-10039-y,Journal of Science Education and Technology,,artificial intelligence,journal,9
31f76619329aba7987394ccb8cac6c9a6dd58a56,Managing artificial intelligence,2023.0,,10.1002/jwmg.22492,Journal of Wildlife Management,,artificial intelligence,journal,8
e936f248b2c0489316ed1521656af2564c3502c3,The FAIR Guiding Principles for scientific data management and stewardship,2016.0,"There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.",10.1038/sdata.2016.18,Scientific Data,"Medicine,Computer Science,Geography",data management,workshop,13
8dd0c1e955c66092ff951941a151336211e6e171,PhyloSuite: An integrated and scalable desktop platform for streamlined molecular sequence data management and evolutionary phylogenetics studies,2019.0,"Multigene and genomic data sets have become commonplace in the field of phylogenetics, but many existing tools are not designed for such data sets, which often makes the analysis time‐consuming and tedious. Here, we present PhyloSuite, a (cross‐platform, open‐source, stand‐alone Python graphical user interface) user‐friendly workflow desktop platform dedicated to streamlining molecular sequence data management and evolutionary phylogenetics studies. It uses a plugin‐based system that integrates several phylogenetic and bioinformatic tools, thereby streamlining the entire procedure, from data acquisition to phylogenetic tree annotation (in combination with iTOL). It has the following features: (a) point‐and‐click and drag‐and‐drop graphical user interface; (b) a workplace to manage and organize molecular sequence data and results of analyses; (c) GenBank entry extraction and comparative statistics; and (d) a phylogenetic workflow with batch processing capability, comprising sequence alignment (mafft and macse), alignment optimization (trimAl, HmmCleaner and Gblocks), data set concatenation, best partitioning scheme and best evolutionary model selection (PartitionFinder and modelfinder), and phylogenetic inference (MrBayes and iq‐tree). PhyloSuite is designed for both beginners and experienced researchers, allowing the former to quick‐start their way into phylogenetic analysis, and the latter to conduct, store and manage their work in a streamlined way, and spend more time investigating scientific questions instead of wasting it on transferring files from one software program to another.",10.1111/1755-0998.13096,Molecular Ecology Resources,"Biology,Medicine",data management,workshop,7
04eaa03dc670afa88c9f3c83fc8da08ef4d31cdd,"TM4: a free, open-source system for microarray data management and analysis.",2003.0,"White1, J. Li1, W. Liang1, N. Bhagabati1, J. Braisted1, M. Klapa1, T. Currier1, M. Thiagarajan1, A. Sturn1, M. Snuffin2, A. Rezantsev2, D. Popov2, A. Ryltsov2, E. Kostukovich2, I. Borisovsky2, Z. Liu3, A. Vinsavich3, V. Trush3, and J. Quackenbush1,4 1The Institute for Genomic Research, Rockville, MD, 2DataNaut, Bethesda, MD, 3Syntek Systems, Bethesda, MD, and 4Department of Biochemistry, George Washington University, Washington, D.C., USA",10.2144/03342MT01,BioTechniques,"Computer Science,Medicine",data management,workshop,12
c07802ed8a25998e9bd44ee1ddbcc63b7eb34060,Data Management for Machine Learning: A Survey,2023.0,"Machine learning (ML) has widespread applications and has revolutionized many industries, but suffers from several challenges. First, sufficient high-quality training data is inevitable for producing a well-performed model, but the data is always human expensive to acquire. Second, a large amount of training data and complicated model structures lead to the inefficiency of training and inference. Third, given an ML task, one always needs to train lots of models, which are hard to manage in real applications. Fortunately, database techniques can benefit ML by addressing the above three challenges. In this paper, we review existing studies from the following three aspects along with the pipeline highly related to ML. (1) Data preparation (Pre-ML): it focuses on preparing high-quality training data that can improve the performance of the ML model, where we review data discovery, data cleaning and data labeling. (2) Model training & inference (In-ML): researchers in ML community focus on improving the model performance during training, while in this survey we mainly study how to accelerate the entire training process, also including feature selection and model selection. (3) Model management (Post-ML): in this part, we survey how to store, query, deploy and debug the models after training. Finally, we provide research challenges and future directions.",10.1109/TKDE.2022.3148237,IEEE Transactions on Knowledge and Data Engineering,Computer Science,data management,journal,13
4be9368abc2474d6fd38639e523cf03af1873fd9,From Smart Farming towards Agriculture 5.0: A Review on Crop Data Management,2020.0,"The information that crops offer is turned into profitable decisions only when efficiently managed. Current advances in data management are making Smart Farming grow exponentially as data have become the key element in modern agriculture to help producers with critical decision-making. Valuable advantages appear with objective information acquired through sensors with the aim of maximizing productivity and sustainability. This kind of data-based managed farms rely on data that can increase efficiency by avoiding the misuse of resources and the pollution of the environment. Data-driven agriculture, with the help of robotic solutions incorporating artificial intelligent techniques, sets the grounds for the sustainable agriculture of the future. This paper reviews the current status of advanced farm management systems by revisiting each crucial step, from data acquisition in crop fields to variable rate applications, so that growers can make optimized decisions to save money while protecting the environment and transforming how food will be produced to sustainably match the forthcoming population growth.",10.3390/agronomy10020207,Agronomy,Business,data management,workshop,8
6548106035c7208ad498730627874a482734b9ac,"Blockchain for healthcare data management: opportunities, challenges, and future recommendations",2021.0,,10.1007/s00521-020-05519-w,Neural computing & applications (Print),"Computer Science,Business",data management,workshop,10
82e1e8b222aeaca19d45375b31fdc825d1a821b8,An Overview of Data Warehouse and Data Lake in Modern Enterprise Data Management,2022.0,"Data is the lifeblood of any organization. In today’s world, organizations recognize the vital role of data in modern business intelligence systems for making meaningful decisions and staying competitive in the field. Efficient and optimal data analytics provides a competitive edge to its performance and services. Major organizations generate, collect and process vast amounts of data, falling under the category of big data. Managing and analyzing the sheer volume and variety of big data is a cumbersome process. At the same time, proper utilization of the vast collection of an organization’s information can generate meaningful insights into business tactics. In this regard, two of the popular data management systems in the area of big data analytics (i.e., data warehouse and data lake) act as platforms to accumulate the big data generated and used by organizations. Although seemingly similar, both of them differ in terms of their characteristics and applications. This article presents a detailed overview of the roles of data warehouses and data lakes in modern enterprise data management. We detail the definitions, characteristics and related works for the respective data management frameworks. Furthermore, we explain the architecture and design considerations of the current state of the art. Finally, we provide a perspective on the challenges and promising research directions for the future.",10.3390/bdcc6040132,Big Data and Cognitive Computing,Computer Science,data management,workshop,5
06ba782753bad19254db5d28ad4155556f286ee0,Data Management and Analysis Methods,2000.0,,,,Sociology,data management,workshop,14
28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,Milvus: A Purpose-Built Vector Data Management System,2021.0,"Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.",10.1145/3448016.3457550,SIGMOD Conference,Computer Science,data management,conference,8
c96fc88631f2b8e2fe192027a8a237445635328c,"Pipeline In-Line Inspection Method, Instrumentation and Data Management",2021.0,"Pipelines play an important role in the national/international transportation of natural gas, petroleum products, and other energy resources. Pipelines are set up in different environments and consequently suffer various damage challenges, such as environmental electrochemical reaction, welding defects, and external force damage, etc. Defects like metal loss, pitting, and cracks destroy the pipeline’s integrity and cause serious safety issues. This should be prevented before it occurs to ensure the safe operation of the pipeline. In recent years, different non-destructive testing (NDT) methods have been developed for in-line pipeline inspection. These are magnetic flux leakage (MFL) testing, ultrasonic testing (UT), electromagnetic acoustic technology (EMAT), eddy current testing (EC). Single modality or different kinds of integrated NDT system named Pipeline Inspection Gauge (PIG) or un-piggable robotic inspection systems have been developed. Moreover, data management in conjunction with historic data for condition-based pipeline maintenance becomes important as well. In this study, various inspection methods in association with non-destructive testing are investigated. The state of the art of PIGs, un-piggable robots, as well as instrumental applications, are systematically compared. Furthermore, data models and management are utilized for defect quantification, classification, failure prediction and maintenance. Finally, the challenges, problems, and development trends of pipeline inspection as well as data management are derived and discussed.",10.3390/s21113862,Italian National Conference on Sensors,"Medicine,Computer Science",data management,conference,8
d008893e01fa7f6c5fb01dadf3f97ee96835c303,An Overview of Platforms for Big Earth Observation Data Management and Analysis,2020.0,"In recent years, Earth observation (EO) satellites have generated big amounts of geospatial data that are freely available for society and researchers. This scenario brings challenges for traditional spatial data infrastructures (SDI) to properly store, process, disseminate and analyze these big data sets. To meet these demands, novel technologies have been proposed and developed, based on cloud computing and distributed systems, such as array database systems, MapReduce systems and web services to access and process big Earth observation data. Currently, these technologies have been integrated into cutting edge platforms in order to support a new generation of SDI for big Earth observation data. This paper presents an overview of seven platforms for big Earth observation data management and analysis—Google Earth Engine (GEE), Sentinel Hub, Open Data Cube (ODC), System for Earth Observation Data Access, Processing and Analysis for Land Monitoring (SEPAL), openEO, JEODPP, and pipsCloud. We also provide a comparison of these platforms according to criteria that represent capabilities of the EO community interest.",10.3390/rs12081253,Remote Sensing,Computer Science,data management,workshop,15
0df5a4f9cc8a244715fe9968732497d2ac2a7cd1,"A Survey on Trajectory Data Management, Analytics, and Learning",2020.0,"Recent advances in sensor and mobile devices have enabled an unprecedented increase in the availability and collection of urban trajectory data, thus increasing the demand for more efficient ways to manage and analyze the data being produced. In this survey, we comprehensively review recent research trends in trajectory data management, ranging from trajectory pre-processing, storage, common trajectory analytic tools, such as querying spatial-only and spatial-textual trajectory data, and trajectory clustering. We also explore four closely related analytical tasks commonly used with trajectory data in interactive or real-time processing. Deep trajectory learning is also reviewed for the first time. Finally, we outline the essential qualities that a trajectory data management system should possess to maximize flexibility.",10.1145/3440207,ACM Computing Surveys,Computer Science,data management,journal,6
0f38b3d717e0fcc6eacc9c6e78b252227440e04e,A Survey of Blockchain Data Management Systems,2021.0,"Blockchain has been widely deployed in various fields, such as finance, education, and public services. Blockchain has decentralized mechanisms with persistency and auditability and runs as an immutable distributed ledger, where transactions are jointly performed through cryptocurrency-based consensus algorithms by worldwide distributed nodes. There have been many survey papers reviewing the blockchain technologies from different perspectives, e.g., digital currencies, consensus algorithms, and smart contracts. However, none of them have focused on the blockchain data management systems. To fill in this gap, we have conducted a comprehensive survey on the data management systems, based on three typical types of blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed Acyclic Graph)-based blockchain. We categorize their data management mechanisms into three layers: blockchain architecture, blockchain data structure, and blockchain storage engine, where block architecture indicates how to record transactions on a distributed ledger, blockchain data structure refers to the internal structure of each block, and blockchain storage engine specifies the storage form of data on the blockchain system. For each layer, the works advancing the state-of-the-art are discussed together with technical challenges. Furthermore, we lay out several possible future research directions for the blockchain data management systems.",10.1145/3502741,ACM Transactions on Embedded Computing Systems,Computer Science,data management,journal,9
1ff76ab0fcf22110df62337d462e15d79a2a2593,A Blockchain-Based Trusted Data Management Scheme in Edge Computing,2020.0,"With rapid development of computing technologies, large amount of data are gathered from edge terminals or Internet of Things (IoT) devices, however data trust and security in edge computing environment are very important issues to be considered, especially when the gathered data are fraud or dishonest, or the data are misused or spread without any authorization, which may lead to serious problems. In this article, a blockchain-based trusted data management scheme (called BlockTDM) in edge computing is proposed to solve the above problems, in which we proposed a flexible and configurable blockchain architecture that includes mutual authentication protocol, flexible consensus, smart contract, block and transaction data management, blockchain nodes management, and deployment. The BlockTDM scheme can support matrix-based multichannel data segment and isolation for sensitive or privacy data protection, and moreover, we have designed user-defined sensitive data encryption before the transaction payload stores in blockchain system, and have implemented conditional access and decryption query of the protected blockchain data and transactions through smart contract. Finally, we have evaluated the proposed BlockTDM scheme security, availability, and efficiency with large amount of experiments. Analysis and evaluations manifest that the proposed BlockTDM scheme provides a general, flexible, and configurable blockchain-based paradigm for trusted data management with tamper-resistance, which is suitable for edge computing with high-level security and creditability.",10.1109/TII.2019.2933482,IEEE Transactions on Industrial Informatics,Computer Science,data management,journal,7
e8b7a9be9f2d0578a95319ed5841978e10429967,Big data management in the mining industry,2020.0,,10.1007/s12613-019-1937-z,"International Journal of Minerals, Metallurgy, and Materials",Business,data management,journal,12
799d5a8271887adede035644d878c7bd555576df,Geospatial Data Management Research: Progress and Future Directions,2020.0,"Without geospatial data management, today’s challenges in big data applications such as earth observation, geographic information system/building information modeling (GIS/BIM) integration, and 3D/4D city planning cannot be solved. Furthermore, geospatial data management plays a connecting role between data acquisition, data modelling, data visualization, and data analysis. It enables the continuous availability of geospatial data and the replicability of geospatial data analysis. In the first part of this article, five milestones of geospatial data management research are presented that were achieved during the last decade. The first one reflects advancements in BIM/GIS integration at data, process, and application levels. The second milestone presents theoretical progress by introducing topology as a key concept of geospatial data management. In the third milestone, 3D/4D geospatial data management is described as a key concept for city modelling, including subsurface models. Progress in modelling and visualization of massive geospatial features on web platforms is the fourth milestone which includes discrete global grid systems as an alternative geospatial reference framework. The intensive use of geosensor data sources is the fifth milestone which opens the way to parallel data storage platforms supporting data analysis on geosensors. In the second part of this article, five future directions of geospatial data management research are presented that have the potential to become key research fields of geospatial data management in the next decade. Geo-data science will have the task to extract knowledge from unstructured and structured geospatial data and to bridge the gap between modern information technology concepts and the geo-related sciences. Topology is presented as a powerful and general concept to analyze GIS and BIM data structures and spatial relations that will be of great importance in emerging applications such as smart cities and digital twins. Data-streaming libraries and “in-situ” geo-computing on objects executed directly on the sensors will revolutionize geo-information science and bridge geo-computing with geospatial data management. Advanced geospatial data visualization on web platforms will enable the representation of dynamically changing geospatial features or moving objects’ trajectories. Finally, geospatial data management will support big geospatial data analysis, and graph databases are expected to experience a revival on top of parallel and distributed data stores supporting big geospatial data analysis.",10.3390/ijgi9020095,ISPRS Int. J. Geo Inf.,Computer Science,data management,workshop,10
b795c74a0150ec091003ffbaa5bd7d74487c137b,Responsible data management,2020.0,"The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people's lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data. In this article, we argue that the data management community is uniquely positioned to lead the responsible design, development, use, and oversight of ADS. We outline a technical research agenda that requires that we step outside our comfort zone of engineering for efficiency and accuracy, to also incorporate reasoning about values and beliefs. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.",10.14778/3415478.3415570,Proceedings of the VLDB Endowment,Computer Science,data management,workshop,14
8d67b76222d84dcd337b8a2c78f13837070a79ce,Blockchain-based data management for digital twin of product,2020.0,,10.1016/j.jmsy.2020.01.009,,Computer Science,data management,workshop,8
da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f,Big data management and environmental performance: role of big data decision-making capabilities and decision-making quality,2020.0,"PurposeThis study is undertaken to examine the antecedents and role of big data decision-making capabilities toward decision-making quality and environmental performance among the Chinese public and private hospitals. It also examined the moderating effect of big data governance that was almost ignored in previous studies.Design/methodology/approachThe target population consisted of managerial employees (IT experts and executives) in hospitals. Data collected using a survey questionnaire from 752 respondents (374 respondents from public hospitals and 378 respondents from private hospitals) was subjected to PLS-SEM for analysis.FindingsFindings revealed that data management challenges (leadership focus, talent management, technology and organizational culture for big data) are significant antecedents for big data decision-making capabilities in both public and private hospitals. Moreover, it was also found that big data decision-making capabilities played a key role to improve the decision-making quality (effectiveness and efficiency), which positively contribute toward environmental performance in public and private hospitals of China. Public hospitals are playing greater attention to big data management for the sake of quality decision-making and environmental performance than private hospitals.Practical implicationsThis study provides guidelines required by hospitals to strengthen their big data capabilities to improve decision-making quality and environmental performance.Originality/valueThe proposed model provides an insight look at the dynamic capabilities theory in the domain of big data management to tackle the environmental issues in hospitals. The current study is the novel addition in the literature, and it identifies that big data capabilities are envisioned to be a game-changer player in effective decision-making and to improve the environmental performance in health sector.",10.1108/jeim-04-2020-0137,Journal of Enterprise Information Management,Computer Science,data management,journal,12
ff74bfbd9ebf4c54809873aecb04be27e9402cb8,Data management for developing digital twin ontology model,2020.0,"Digital Twin (DT) is the imitation of the real world product, process or system. Digital Twin is the ideal solution for data-driven optimisations in different phases of the product lifecycle. With the rapid growth in DT research, data management for digital twin is a challenging field for both industries and academia. The challenges for DT data management are analysed in this article are data variety, big data & data mining and DT dynamics. The current research proposes a novel concept of DT ontology model and methodology to address these data management challenges. The DT ontology model captures and models the conceptual knowledge of the DT domain. Using the proposed methodology, such domain knowledge is transformed into a minimum data model structure to map, query and manage databases for DT applications. The proposed research is further validated using a case study based on Condition-Based Monitoring (CBM) DT application. The query formulation around minimum data model structure further shows the effectiveness of the current approach by returning accurate results, along with maintaining semantics and conceptual relationships along DT lifecycle. The method not only provides flexibility to retain knowledge along DT lifecycle but also helps users and developers to design, maintain and query databases effectively for DT applications and systems of different scale and complexities.",10.1177/0954405420978117,"Proceedings of the Institution of mechanical engineers. Part B, journal of engineering manufacture",Computer Science,data management,journal,8
d5f169880e30e1f76827d72f862555d00b01bed9,A vector space model for automatic indexing,1975.0,"In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.",10.1145/361219.361220,CACM,Computer Science,indexing,journal,13
5b1e1696564e5a3021ac3a501c9deeb6c0fbc637,Color indexing,1991.0,,10.1007/BF00130487,International Journal of Computer Vision,Computer Science,indexing,journal,7
7fb7364cf5f9bd48a9dd3a92a5243aae128afc24,Exact indexing of dynamic time warping,2002.0,,10.1007/s10115-004-0154-9,Knowledge and Information Systems,"Mathematics,Computer Science",indexing,workshop,8
8d94832e245906775b428e949ac1f635bfb28ad3,Managing Gigabytes: Compressing and Indexing Documents and Images,1999.0,PREFACE 1. OVERVIEW 2. TEXT COMPRESSION 3. INDEXING 4. QUERYING 5. INDEX CONSTRUCTION 6. IMAGE COMPRESSION 7. TEXTUAL IMAGES 8. MIXED TEXT AND IMAGES 9. IMPLEMENTATION 10. THE INFORMATION EXPLOSION A. GUIDE TO THE MG SYSTEM B. GUIDE TO THE NZDL REFERENCES INDEX,10.1109/tit.1995.476344,,Computer Science,indexing,workshop,8
e7cf3c1a584b16d5e15e5fbec7c05762385d669e,Droplet-based combinatorial indexing for massive-scale single-cell chromatin accessibility,2019.0,,10.1038/s41587-019-0147-6,Nature Biotechnology,"Medicine,Computer Science",indexing,journal,13
69a2479a49154e3bd51e44e636ab5692ed20b142,Probabilistic Latent Semantic Indexing,1999.0,"Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.",10.1145/3130348.3130370,SIGIR Forum,Computer Science,indexing,workshop,5
9fb01cd85317f84fb4def982a81f19da2571bae1,Image indexing using color correlograms,1997.0,"We define a new image feature called the color correlogram and use it for image indexing and comparison. This feature distills the spatial correlation of colors, and is both effective and inexpensive for content-based image retrieval. The correlogram robustly tolerates large changes in appearance and shape caused by changes in viewing positions, camera zooms, etc. Experimental evidence suggests that this new feature outperforms not only the traditional color histogram method but also the recently proposed histogram refinement methods for image indexing/retrieval.",10.1109/CVPR.1997.609412,Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition,"Computer Science,Mathematics",indexing,conference,10
5fe8ed6e2c75a3b352be695f423cc2fac19446fa,Multiplex single-cell profiling of chromatin accessibility by combinatorial cellular indexing,2015.0,"Chromatin state and the single cell Identifying the chromatin state of any single cell, which may or may not have a different function or represent different stages relative to others collected within any single culture, experiment, or tissue, has been challenging. Cusanovitch et al. skirted previously identified technological limitations to identify regions of accessible chromatin at single-cell resolution. Combinatorial cellular indexing, a strategy for multiplex barcoding of thousands of single cells per experiment, was successfully used to investigate the genome-wide chromatin accessibility landscape in each of over 15,000 single cells. Science, this issue p. 910 Combinatorial indexing can identify chromatin states at single-cell resolution. Technical advances have enabled the collection of genome and transcriptome data sets with single-cell resolution. However, single-cell characterization of the epigenome has remained challenging. Furthermore, because cells must be physically separated before biochemical processing, conventional single-cell preparatory methods scale linearly. We applied combinatorial cellular indexing to measure chromatin accessibility in thousands of single cells per assay, circumventing the need for compartmentalization of individual cells. We report chromatin accessibility profiles from more than 15,000 single cells and use these data to cluster cells on the basis of chromatin accessibility landscapes. We identify modules of coordinately regulated chromatin accessibility at the level of single cells both between and within cell types, with a scalable method that may accelerate progress toward a human cell atlas.",10.1126/science.aab1601,Science,"Medicine,Biology",indexing,workshop,7
7f8859a57e35c66164678710187d4374822411d8,An improved dual-indexing approach for multiplexed 16S rRNA gene sequencing on the Illumina MiSeq platform,2014.0,To take advantage of affordable high-throughput next-generation sequencing technologies to characterize microbial community composition often requires the development of improved methods to overcome technical limitations inherent to the sequencing platforms. Sequencing low sequence diversity libraries such as 16S rRNA amplicons has been problematic on the Illumina MiSeq platform and often generates sequences of suboptimal quality. Here we present an improved dual-indexing amplification and sequencing approach to assess the composition of microbial communities from clinical samples using the V3-V4 region of the 16S rRNA gene on the Illumina MiSeq platform. We introduced a 0 to 7 bp “heterogeneity spacer” to the index sequence that allows an equal proportion of samples to be sequenced out of phase. Our approach yields high quality sequence data from 16S rRNA gene amplicons using both 250 bp and 300 bp paired-end MiSeq protocols and provides a flexible and cost-effective sequencing option.,10.1186/2049-2618-2-6,Microbiome,"Medicine,Biology",indexing,workshop,8
c4dd9a19d822c965ce8cde55ab23b8a0b628278a,An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition,2015.0,"This article provides an overview of the first BioASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March and September 2013. BioASQ assesses the ability of systems to semantically index very large numbers of biomedical scientific articles, and to return concise and user-understandable answers to given natural language questions by combining information from biomedical articles and ontologies. The 2013 BioASQ competition comprised two tasks, Task 1a and Task 1b. In Task 1a participants were asked to automatically annotate new PubMed documents with MeSH headings. Twelve teams participated in Task 1a, with a total of 46 system runs submitted, and one of the teams performing consistently better than the MTI indexer used by NLM to suggest MeSH headings to curators. Task 1b used benchmark datasets containing 29 development and 282 test English questions, along with gold standard (reference) answers, prepared by a team of biomedical experts from around Europe and participants had to automatically produce answers. Three teams participated in Task 1b, with 11 system runs. The BioASQ infrastructure, including benchmark datasets, evaluation mechanisms, and the results of the participants and baseline methods, is publicly available. A publicly available evaluation infrastructure for biomedical semantic indexing and QA has been developed, which includes benchmark datasets, and can be used to evaluate systems that: assign MeSH headings to published articles or to English questions; retrieve relevant RDF triples from ontologies, relevant articles and snippets from PubMed Central; produce “exact” and paragraph-sized “ideal” answers (summaries). The results of the systems that participated in the 2013 BioASQ competition are promising. In Task 1a one of the systems performed consistently better from the NLM’s MTI indexer. In Task 1b the systems received high scores in the manual evaluation of the “ideal” answers; hence, they produced high quality summaries as answers. Overall, BioASQ helped obtain a unified view of how techniques from text classification, semantic indexing, document and passage retrieval, question answering, and text summarization can be combined to allow biomedical experts to obtain concise, user-understandable answers to questions reflecting their real information needs.",10.1186/s12859-015-0564-6,BMC Bioinformatics,"Computer Science,Medicine",indexing,workshop,15
69cad06dc26140ef3a7c19f05cbd56a2d04fad1a,Automated identification and indexing of dislocations in crystal interfaces,2012.0,"We present a computational method for identifying partial and interfacial dislocations in atomistic models of crystals with defects. Our automated algorithm is based on a discrete Burgers circuit integral over the elastic displacement field and is not limited to specific lattices or dislocation types. Dislocations in grain boundaries and other interfaces are identified by mapping atomic bonds from the dislocated interface to an ideal template configuration of the coherent interface to reveal incompatible displacements induced by dislocations and to determine their Burgers vectors. In addition, the algorithm generates a continuous line representation of each dislocation segment in the crystal and also identifies dislocation junctions.",10.1088/0965-0393/20/8/085007,,"Physics,Materials Science",indexing,workshop,11
80e850ae69e141eab2de754998cb2f617a7c73ef,Citation Indexing Its Theory And Application In Science Technology And Humanities,2016.0,,,,Computer Science,indexing,workshop,12
75b868e844e58db707fb5dbf7acbe2e26ba8c122,A survey on indexing techniques for big data: taxonomy and performance evaluation,2016.0,,10.1007/s10115-015-0830-y,Knowledge and Information Systems,Computer Science,indexing,workshop,7
c2872fb23b02597034a179f4adb82a00d6ffda8d,Probabilistic latent semantic indexing,1999.0,"Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{speci c synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and de nes a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with di erent dimensionalities has proven to be advantageous.",10.1145/312624.312649,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,Computer Science,indexing,conference,8
ddeea66c9550f99f9a6768d4b240a9fe9957487d,Indexing based on scale invariant interest points,2001.0,"This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: (1) Interest points can be adapted to scale and give repeatable results (geometrically stable). (2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors are, in addition, invariant to image rotation, of affine illumination changes and robust to small perspective deformations. Experimental results for indexing show an excellent performance up to a scale factor of 4 for a database with more than 5000 images.",10.1109/ICCV.2001.937561,Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001,"Computer Science,Mathematics",indexing,conference,10
b538999f458a12df98d197befcc4426525cf8237,Efficient Indexing of Billion-Scale Datasets of Deep Descriptors,2016.0,"Existing billion-scale nearest neighbor search systems have mostly been compared on a single dataset of a billion of SIFT vectors, where systems based on the Inverted Multi-Index (IMI) have been performing very well, achieving state-of-the-art recall in several milliseconds. SIFT-like descriptors, however, are quickly being replaced with descriptors based on deep neural networks (DNN) that provide better performance for many computer vision tasks. In this paper, we introduce a new dataset of one billion descriptors based on DNNs and reveal the relative inefficiency of IMI-based indexing for such descriptors compared to SIFT data. We then introduce two new indexing structures, the Non-Orthogonal Inverted Multi-Index (NO-IMI) and the Generalized Non-Orthogonal Inverted Multi-Index (GNO-IMI). We show that due to additional flexibility, the new structures are able to adapt to DNN descriptor distribution in a better way. In particular, extensive experiments on the new dataset demonstrate that these data structures provide considerably better trade-off between the speed of retrieval and recall, given similar amount of memory, as compared to the standard Inverted Multi-Index.",10.1109/CVPR.2016.226,Computer Vision and Pattern Recognition,Computer Science,indexing,workshop,14
728211a7edfe536299670f34714de1b426d969dc,Locally adaptive dimensionality reduction for indexing large time series databases,2001.0,"Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data.. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower bounding, but very tight Euclidean distance approximation and show how they can support fast exact searching, and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.",10.1145/375663.375680,ACM SIGMOD Conference,Computer Science,indexing,conference,12
896be512ce514903d05357335faaf69a5cf8ce85,Indexing and Active Fund Management: International Evidence,2015.0,"We examine the relation between indexing and active management in the mutual fund industry worldwide. Explicit indexing and closet indexing by active funds are associated with countries’ regulatory and financial market environments. We find that actively managed funds are more active and charge lower fees when they face more competitive pressure from low-cost explicitly indexed funds. A quasi-natural experiment using the exogenous variation in indexed funds generated by the passage of pension laws supports a causal interpretation of the results. Moreover, the average alpha generated by active management is higher in countries with more explicit indexing and lower in countries with more closet indexing. Overall, our evidence suggests that explicit indexing improves competition in the mutual fund industry.",10.2139/ssrn.1830207,,Economics,indexing,workshop,10
f39d3e88cce063ccd3ca01100efd44dcabc9d3b4,Automatic Linguistic Indexing of Pictures by a Statistical Modeling Approach,2003.0,"Automatic linguistic indexing of pictures is an important but highly challenging problem for researchers in computer vision and content-based image retrieval. In this paper, we introduce a statistical modeling approach to this problem. Categorized images are used to train a dictionary of hundreds of statistical models each representing a concept. Images of any given concept are regarded as instances of a stochastic process that characterizes the concept. To measure the extent of association between an image and the textual description of a concept, the likelihood of the occurrence of the image based on the characterizing stochastic process is computed. A high likelihood indicates a strong association. In our experimental implementation, we focus on a particular group of stochastic processes, that is, the two-dimensional multiresolution hidden Markov models (2D MHMMs). We implemented and tested our ALIP (Automatic Linguistic Indexing of Pictures) system on a photographic image database of 600 different concepts, each with about 40 training images. The system is evaluated quantitatively using more than 4,600 images outside the training database and compared with a random annotation scheme. Experiments have demonstrated the good accuracy of the system and its high potential in linguistic indexing of photographic images.",10.1109/TPAMI.2003.1227984,IEEE Transactions on Pattern Analysis and Machine Intelligence,Computer Science,indexing,journal,5
0310ff23ec2c0be81ea1af02f9e1fd6672d69f1d,Powder pattern indexing with the dichotomy method,2004.0,"The efficiency of the successive dichotomy method for powder diffraction pattern indexing [Louer & Louer (1972). J. Appl. Cryst. 5, 271–275] has been proved over more than 30 years of usage. Features implemented in the new version of the computer program DICVOL04 include (i) a tolerance to the presence of impurity (or inaccurately measured) diffraction lines, (ii) a refinement of the `zero-point' position, (iii) a reviewing of all input lines from the solution found from, generally, the first 20 lines, (iv) a cell analysis, based on the concept of the reduced cell, to identify equivalent monoclinic and triclinic solutions, and (v) an optional analysis of input powder data to detect the presence of a significant `zero-point' offset. New search strategies have also been introduced, e.g. each crystal system is scanned separately, within the input volume limits, to limit the risk of missing a solution characterized by a metric lattice singularity. The default values in the input file have been extended to 25 A for the linear parameters and 2500 A3 for the cell volume. The search is carried out exhaustively within the input parameter limits and the absolute error on peak position measurements. Many tests with data from the literature and from powder data of pharmaceutical materials, collected with the capillary technique and laboratory monochromatic X-rays, have been performed with a high success rate, covering all crystal symmetries from cubic to triclinic. Some examples reported as `difficult' cases are also discussed. Additionally, a few recommendations for the correct practice of powder pattern indexing are reported.",10.1107/S0021889804014876,,Chemistry,indexing,workshop,6
a2ec47b9bcc95d2456a8a42199233e5d9129ef18,TabTransformer: Tabular Data Modeling Using Contextual Embeddings,2020.0,"We propose TabTransformer, a novel deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-the-art deep learning methods for tabular data by at least 1.0% on mean AUC, and matches the performance of tree-based ensemble models. Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability. Lastly, for the semi-supervised setting we develop an unsupervised pre-training procedure to learn data-driven contextual embeddings, resulting in an average 2.1% AUC lift over the state-of-the-art methods.",,arXiv.org,Computer Science,data modeling,workshop,10
aea731e7cf33aa3d482b13f42cedbc1adb3271c6,"The “Problem” of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation",2022.0,"Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that there exists a *ground truth*, and neglects that there exists genuine human variation in labeling due to disagreement, subjectivity in annotation or multiple plausible answers.In this position paper, we argue that this big open problem of human label variation persists and critically needs more attention to move our field forward. This is because human label variation impacts all stages of the ML pipeline: *data, modeling and evaluation*. However, few works consider all of these dimensions jointly; and existing research is fragmented. We reconcile different previously proposed notions of human label variation, provide a repository of publicly-available datasets with un-aggregated labels, depict approaches proposed so far, identify gaps and suggest ways forward. As datasets are becoming increasingly available, we hope that this synthesized view on the “problem” will lead to an open discussion on possible strategies to devise fundamentally new directions.",10.18653/v1/2022.emnlp-main.731,Conference on Empirical Methods in Natural Language Processing,Computer Science,data modeling,conference,5
cd8a9914d50b0ac63315872530274d158d6aff09,Modeling Relational Data with Graph Convolutional Networks,2017.0,"Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",10.1007/978-3-319-93417-4_38,Extended Semantic Web Conference,"Computer Science,Mathematics",data modeling,conference,9
a85c45ce7c893388e8eafa8a653b042e1497db48,Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling,2021.0,"Vast amount of data generated from networks of sensors, wearables, and the Internet of Things (IoT) devices underscores the need for advanced modeling techniques that leverage the spatio-temporal structure of decentralized data due to the need for edge computation and licensing (data access) issues. While federated learning (FL) has emerged as a framework for model training without requiring direct data sharing and exchange, effectively modeling the complex spatio-temporal dependencies to improve forecasting capabilities still remains an open problem. On the other hand, state-of-the-art spatio-temporal forecasting models assume unfettered access to the data, neglecting constraints on data sharing. To bridge this gap, we propose a federated spatio-temporal model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly encodes the underlying graph structure using graph neural network (GNN)-based architecture under the constraint of cross-node federated learning, which requires that data in a network of nodes is generated locally on each node and remains decentralized. CNFGNN operates by disentangling the temporal dynamics modeling on devices and spatial dynamics on the server, utilizing alternating optimization to reduce the communication cost, facilitating computations on the edge devices. Experiments on the traffic flow forecasting task show that CNFGNN achieves the best forecasting performance in both transductive and inductive learning settings with no extra computation cost on edge devices, while incurring modest communication cost.",10.1145/3447548.3467371,Knowledge Discovery and Data Mining,Computer Science,data modeling,workshop,8
b04550f0722e9614163855ab36fc2430b931a3fe,Bridge condition rating data modeling using deep learning algorithm,2020.0,"Abstract This paper presents a deep learning-based bridge condition rating data modeling approach using selected data from the National Bridge Inventory (NBI) database. The objective of this research is to develop a data-driven approach that enables prediction of future conditions of highway bridge components from historical inspection data. The problem is solved by training a Convolutional Neural Network (CNN) model with online available NBI data. One prominent feature of the CNN model is that if well-trained it can represent the high dimensional data in the dataset abstractions for which conventional mathematical models may be difficult to describe. A case study of Maryland and Delaware highway bridges using historical data (1992–2017) sourced from the NBI database has been performed to demonstrate the proposed method. CNN models for three primary components of these highway bridges including the deck, superstructure, and substructure have been established. Optimization of model parameters is achieved through a parametric study. Research findings suggest that the deep learning model offers a promising tool as a data-driven condition forecasting approach for bridge components with a demonstrated prediction accuracy over 85%.",10.1080/15732479.2020.1712610,Structure and Infrastructure Engineering,Computer Science,data modeling,workshop,6
20a3ed03888037e2802fa9abad02ffa0a8dcc228,The Linked Data Modeling Language (LinkML): A General-Purpose Data Modeling Framework Grounded in Machine-Readable Semantics,2021.0,,,International Conference on Biomedical Ontology,Computer Science,data modeling,conference,8
0456ed94f6c455f99cb67abe8a28aeb3ef9f489b,Neural Networks-Based Aerodynamic Data Modeling: A Comprehensive Review,2020.0,"This paper reviews studies on neural networks in aerodynamic data modeling. In this paper, we analyze the shortcomings of computational fluid dynamics (CFD) and traditional reduced-order models (ROMs). Subsequently, the history and fundamental methodologies of neural networks are introduced. Furthermore, we classify the neural networks based studies in aerodynamic data modeling and illustrate comparisons among them. These studies demonstrate that neural networks are effective approaches to aerodynamic data modeling. Finally, we identify three important trends for future studies in aerodynamic data modeling: a) the transformation method and physics informed models will be combined to solve high-dimensional partial differential equations; b) in the research area of steady aerodynamic response predictions, model-oriented studies and data-integration-oriented studies will become the future research directions, while in unsteady aerodynamic response predictions, radial basis function neural networks (RBFNNs) are the best tools for capturing the nonlinear characteristics of flow data, and convolutional neural networks (CNNs) are expected to replace long short-term memories (LSTMs) to capture the temporal characteristics of flow data; and c) in the field of steady or unsteady flow field reconstructions, the CNN-based conditional generative adversarial networks (cGANs) will be the best frameworks in which to discover the spatiotemporal distribution of flow field data.",10.1109/ACCESS.2020.2993562,IEEE Access,Computer Science,data modeling,journal,8
c0bcd7dc9426a70af15f5ad63b4af92ea4dcbd4d,Data Modeling,2022.0,,10.1201/9781003229629-3,Mastering MySQL for the Web,,data modeling,workshop,14
965359b3008ab50dd04e171551220ec0e7f83aba,Generative Modeling by Estimating Gradients of the Data Distribution,2019.0,"We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.",,Neural Information Processing Systems,"Computer Science,Mathematics",data modeling,workshop,13
8f99f5cb3950fcd1628c6a563be4b4fc4966fa66,Robust Broad Learning System for Uncertain Data Modeling,2018.0,"Broad Learning System (BLS) has achieved good performance in classification and regression problems, and the computational efficiency is especially outstanding. However, there exists various outliers or noise in the sampling data, which puts a robust requirement on the algorithms. Standard BLS is sensitive to the contaminated data because of its composition structure. In this paper, we propose a robust version of BLS called RBLS to improve its generalization on contaminated data modeling. In RBLS, the ℓ2-norm based cost function will be replaced by ℓ1-norm style cost function. The Augmented Lagrange Multiplier (ALM) method is applied to optimize the new model iteratively. The experiments on function approximation and real-world regression demonstrated that the RBLS method has better modeling performance for sampling data with outliers or noise.",10.1109/SMC.2018.00596,"IEEE International Conference on Systems, Man and Cybernetics",Computer Science,data modeling,conference,11
417326e51d78ba8bd2621f23e539b41bbdd336d6,Dynamic Probabilistic Latent Variable Model for Process Data Modeling and Regression Application,2019.0,"Dynamic and uncertainty are two main features of the industrial process data which should be paid attention when carrying out process data modeling and analytics. In this paper, the dynamical and uncertain data characteristics are both taken into consideration for the regression modeling purpose. Based on the probabilistic latent variable modeling framework, the linear dynamic system is introduced for incorporation of the dynamical data feature. The expectation–maximization Algorithm is introduced for parameter learning of the dynamical probabilistic latent variable model, based on which a new soft sensing scheme is then formulated for online prediction of key/quality variables in the process. An industrial case study illustrates the necessity and effectiveness of introducing the dynamical data information into the probabilistic latent variable model.",10.1109/TCST.2017.2767022,IEEE Transactions on Control Systems Technology,Computer Science,data modeling,journal,9
2582ab7c70c9e7fcb84545944eba8f3a7f253248,Translating Embeddings for Modeling Multi-relational Data,2013.0,,,Neural Information Processing Systems,"Computer Science,Mathematics",data modeling,workshop,7
627d7d631fd4e0e2179f82199f014deb7ff0ea0b,Bayesian synthesis of probabilistic programs for automatic data modeling,2019.0,"We present new techniques for automatically constructing probabilistic programs for data analysis, interpretation, and prediction. These techniques work with probabilistic domain-specific data modeling languages that capture key properties of a broad class of data generating processes, using Bayesian inference to synthesize probabilistic programs in these modeling languages given observed data. We provide a precise formulation of Bayesian synthesis for automatic data modeling that identifies sufficient conditions for the resulting synthesis procedure to be sound. We also derive a general class of synthesis algorithms for domain-specific languages specified by probabilistic context-free grammars and establish the soundness of our approach for these languages. We apply the techniques to automatically synthesize probabilistic programs for time series data and multivariate tabular data. We show how to analyze the structure of the synthesized programs to compute, for key qualitative properties of interest, the probability that the underlying data generating process exhibits each of these properties. Second, we translate probabilistic programs in the domain-specific language into probabilistic programs in Venture, a general-purpose probabilistic programming system. The translated Venture programs are then executed to obtain predictions of new time series data and new multivariate data records. Experimental results show that our techniques can accurately infer qualitative structure in multiple real-world data sets and outperform standard data analysis methods in forecasting and predicting new data.",10.1145/3290350,Proc. ACM Program. Lang.,"Computer Science,Mathematics",data modeling,journal,13
0ac0025529c1f9056036be43c561ba67fb8d12a5,Applied Longitudinal Data Analysis Modeling Change And Event Occurrence,2016.0,,,,Computer Science,data modeling,workshop,9
ad10ef93675513a68b93d54f3a461160b53318a3,Data modeling,2021.0,,10.4324/9781003011163-9,Creating Value with Data Analytics in Marketing,,data modeling,workshop,8
e3b94a5f28522e6825aff16ff07d56bd70d26c96,The YANG 1.1 Data Modeling Language,2016.0,"YANG is a data modeling language used to model configuration data, state data, Remote Procedure Calls, and notifications for network management protocols. This document describes the syntax and semantics of version 1.1 of the YANG language. YANG version 1.1 is a maintenance release of the YANG language, addressing ambiguities and defects in the original specification. There are a small number of backward incompatibilities from YANG version 1. This document also specifies the YANG mappings to the Network Configuration Protocol (NETCONF).",10.17487/RFC7950,Request for Comments,Computer Science,data modeling,workshop,7
d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5,An Effective and Scalable Data Modeling for Enterprise Big Data Platform,2019.0,"The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.",10.1109/BigData47090.2019.9005614,2019 IEEE International Conference on Big Data (Big Data),Computer Science,data modeling,conference,14
a0afa4ec10b2b060aff0ec676a661c8013c7df1d,Hierarchical Modeling and Analysis for Spatial Data,2003.0,OVERVIEW OF SPATIAL DATA PROBLEMS Introduction to Spatial Data and Models Fundamentals of Cartography Exercises BASICS OF POINT-REFERENCED DATA MODELS Elements of Point-Referenced Modeling Spatial Process Models Exploratory Approaches for Point-Referenced Data Classical Spatial Prediction Computer Tutorials Exercises BASICS OF AREAL DATA MODELS Exploratory Approaches for Areal Data Brook's Lemma and Markov Random Fields Conditionally Autoregressive (CAR) Models Simultaneous Autoregressive (SAR) Models Computer Tutorials Exercises BASICS OF BAYESIAN INFERENCE Introduction to Hierarchical Modeling and Bayes Theorem Bayesian Inference Bayesian Computation Computer Tutorials Exercises HIERARCHICAL MODELING FOR UNIVARIATE SPATIAL DATA Stationary Spatial Process Models Generalized Linear Spatial Process Modeling Nonstationary Spatial Process Models Areal Data Models General Linear Areal Data Modeling Exercises SPATIAL MISALIGNMENT Point-Level Modeling Nested Block-Level Modeling Nonnested Block-Level Modeling Misaligned Regression Modeling Exercises MULTIVARIATE SPATIAL MODELING Separable Models Coregionalization Models Other Constructive Approaches Multivariate Models for Areal Data Exercises SPATIOTEMPORAL MODELING General Modeling Formulation Point-Level Modeling with Continuous Time Nonseparable Spatio-Temporal Models Dynamic Spatio-Temporal Models Block-Level Modeling Exercises SPATIAL SURVIVAL MODELS Parametric Models Semiparametric Models Spatio-Temporal Models Multivariate Models Spatial Cure Rate Models Exercises SPECIAL TOPICS IN SPATIAL PROCESS MODELING Process Smoothness Revisited Spatially Varying Coefficient Models Spatial CDFs APPENDICES Matrix Theory and Spatial Computing Methods Answers to Selected Exercises REFERENCES AUTHOR INDEX SUBJECT INDEX Short TOC,10.1201/9780203487808,,Computer Science,data modeling,workshop,13
63adc1e5086481e36b19b62707a96b799da51e59,Data modeling versus simulation modeling in the big data era: case study of a greenhouse control system,2017.0,"Recently, big data has received greater attention in diverse research fields, including medicine, science, engineering, management, defense, politics, and others. Such research uses big data to predict target systems, thereby constructing a model of the system in two ways: data modeling and simulation modeling. Data modeling is a method in which a model represents correlation relationships between one set of data and the other set of data. On the other hand, physics-based simulation modeling (or simply simulation modeling) is a more classical, but more powerful, method in which a model represents causal relationships between a set of controlled inputs and corresponding outputs. This paper (i) clarifies the difference between the two modeling approaches, (ii) explains their advantages and limitations and compares each characteristic, and (iii) presents a complementary cooperation modeling approach. Then, we apply the proposed modeling to develop a greenhouse control system in the real world. Finally, we expect that this modeling approach will be an alternative modeling approach in the big data era.",10.1177/0037549717692866,International Conference on Advances in System Simulation,"Engineering,Computer Science",data modeling,conference,12
96023195e889fc258e6ff30aa99d250982dfae01,Digital Twin Data Modeling with AutomationML and a Communication Methodology for Data Exchange,2016.0,,10.1016/J.IFACOL.2016.11.115,,Engineering,data modeling,workshop,10
ccca203382e5dd198c089a0f1d7af7bef0f694e9,TBtools - an integrative toolkit developed for interactive analyses of big biological data.,2020.0,,10.1016/j.molp.2020.06.009,Molecular Plant,"Medicine,Biology",big data,workshop,13
91b63db746becca15090963a8990dfe2b5103799,"Big data: The next frontier for innovation, competition, and productivity",2011.0,,,,Business,big data,workshop,8
f117c6f12d067bd66dad40996b3931c069daa2da,Business Intelligence and Analytics: From Big Data to Big Impact,2012.0,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",10.2307/41703503,MIS Q.,"Computer Science,Engineering",big data,workshop,12
bf5a42b53d156c0811e88e60d2a49f9fd9367cae,Big data: the management revolution.,2012.0,,,Harvard Business Review,"Computer Science,Medicine",big data,workshop,8
4b06c7e29280b1c6bc05c9df39023b48fef02c93,Escaping the Big Data Paradigm with Compact Transformers,2021.0,"With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",,arXiv.org,Computer Science,big data,workshop,12
85328b4a8132bf4299f8cd7f8e79e850d561c8fc,Big Data Analytics: A Survey,2022.0,"Internet-based programs and communication techniques have become widely used and respected in the IT industry recently. A persistent source of ""big data,"" or data that is enormous in volume, diverse in type, and has a complicated multidimensional structure, is internet applications and communications. Today, several measures are routinely performed with no assurance that any of them will be helpful in understanding the phenomenon of interest in an era of automatic, large-scale data collection. Online transactions that involve buying, selling, or even investing are all examples of e-commerce. As a result, they generate data that has a complex structure and a high dimension. The usual data storage techniques cannot handle those enormous volumes of data. There is a lot of work being done to find ways to minimize the dimensionality of big data in order to provide analytics reports that are even more accurate and data visualizations that are more interesting. As a result, the purpose of this survey study is to give an overview of big data analytics along with related problems and issues that go beyond technology.",10.25195/ijci.v49i1.384,Iraqi Journal for Computers and Informatics,,big data,journal,15
41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1,The use of Big Data Analytics in healthcare,2022.0,"The introduction of Big Data Analytics (BDA) in healthcare will allow to use new technologies both in treatment of patients and health management. The paper aims at analyzing the possibilities of using Big Data Analytics in healthcare. The research is based on a critical analysis of the literature, as well as the presentation of selected results of direct research on the use of Big Data Analytics in medical facilities. The direct research was carried out based on research questionnaire and conducted on a sample of 217 medical facilities in Poland. Literature studies have shown that the use of Big Data Analytics can bring many benefits to medical facilities, while direct research has shown that medical facilities in Poland are moving towards data-based healthcare because they use structured and unstructured data, reach for analytics in the administrative, business and clinical area. The research positively confirmed that medical facilities are working on both structural data and unstructured data. The following kinds and sources of data can be distinguished: from databases, transaction data, unstructured content of emails and documents, data from devices and sensors. However, the use of data from social media is lower as in their activity they reach for analytics, not only in the administrative and business but also in the clinical area. It clearly shows that the decisions made in medical facilities are highly data-driven. The results of the study confirm what has been analyzed in the literature that medical facilities are moving towards data-based healthcare, together with its benefits.",10.1186/s40537-021-00553-4,Journal of Big Data,"Medicine,Computer Science",big data,journal,9
cc017a62c605a0749e35a1264a46d62e78fb68b7,Big Data Analytics,2019.0,,10.1007/978-3-642-35542-4,Lecture Notes in Computer Science,Computer Science,big data,workshop,13
1bc34cb22131554ba18f6ba9e6ede5beb42939f1,"Beyond the hype: Big data concepts, methods, and analytics",2015.0,,10.1016/J.IJINFOMGT.2014.10.007,International Journal of Information Management,Computer Science,big data,journal,15
4e6bba65f7636a655c778a3e54cc58e148468963,CRITICAL QUESTIONS FOR BIG DATA,2012.0,"The era of Big Data has begun. Computer scientists, physicists, economists, mathematicians, political scientists, bio-informaticists, sociologists, and other scholars are clamoring for access to the massive quantities of information produced by and about people, things, and their interactions. Diverse groups argue about the potential benefits and costs of analyzing genetic sequences, social media interactions, health records, phone logs, government records, and other digital traces left by people. Significant questions emerge. Will large-scale search data help us create better tools, services, and public goods? Or will it usher in a new wave of privacy incursions and invasive marketing? Will data analytics help us understand online communities and political movements? Or will it be used to track protesters and suppress speech? Will it transform how we study human communication and culture, or narrow the palette of research options and alter what ‘research’ means? Given the rise of Big Data as a socio-technical phenomenon, we argue that it is necessary to critically interrogate its assumptions and biases. In this article, we offer six provocations to spark conversations about the issues of Big Data: a cultural, technological, and scholarly phenomenon that rests on the interplay of technology, analysis, and mythology that provokes extensive utopian and dystopian rhetoric.",10.1080/1369118X.2012.678878,,Sociology,big data,workshop,10
38f5b53b49be555430f33b8363910191a3df1d14,"A Survey on Big Data Analytics: Challenges, Open Research Issues and Tools",2022.0,"Abstract: A huge repository of terabytes of data is generated each day from modern information systems and digital technologies such as Internet of Things and cloud computing. Analysis of these massive data requires a lot of efforts at multiple levels to extract knowledge for decision making. Therefore, big data analysis is a current area of research and development. The basic objective of this paper is to explore the potential impact of big data challenges, open research issues, and various tools associated with it. As a result, this article provides a platform to explore big data at numerous stages. Additionally, it opens a new horizon for researchers to develop the solution, based on the challenges and open research issues.",10.14569/IJACSA.2016.070267,International Journal for Research in Applied Science and Engineering Technology,Computer Science,big data,journal,12
1d174f0e3c391368d0f3384a144a6c7487f2a143,Big Data's Disparate Impact,2016.0,"Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm’s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Essay examines these concerns through the lens of American antidiscrimination law — more particularly, through Title VII’s prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining’s victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission’s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others’ discrimination against members of protected groups, or flaws in the underlying dataAddressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data’s disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of “discrimination” and “fairness.”",10.2139/SSRN.2477899,,Sociology,big data,workshop,7
3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,2017.0,,10.1080/01972243.2017.1354593,The Information Society,Computer Science,big data,workshop,14
178571a5cde984c895493e2eb6c5487449d055cf,"Data mining in clinical big data: the frequently used databases, steps, and methodological models",2021.0,"Many high quality studies have emerged from public databases, such as Surveillance, Epidemiology, and End Results (SEER), National Health and Nutrition Examination Survey (NHANES), The Cancer Genome Atlas (TCGA), and Medical Information Mart for Intensive Care (MIMIC); however, these data are often characterized by a high degree of dimensional heterogeneity, timeliness, scarcity, irregularity, and other characteristics, resulting in the value of these data not being fully utilized. Data-mining technology has been a frontier field in medical research, as it demonstrates excellent performance in evaluating patient risks and assisting clinical decision-making in building disease-prediction models. Therefore, data mining has unique advantages in clinical big-data research, especially in large-scale medical public databases. This article introduced the main medical public database and described the steps, tasks, and models of data mining in simple language. Additionally, we described data-mining methods along with their practical applications. The goal of this work was to aid clinical researchers in gaining a clear and intuitive understanding of the application of data-mining technology on clinical big-data in order to promote the production of research results that are beneficial to doctors and patients.",10.1186/s40779-021-00338-z,Military Medical Research,Medicine,big data,workshop,11
bf69c98fca9a9f6c1cde871beddbcdc668b77771,"Big Data: A Revolution That Will Transform How We Live, Work, and Think",2015.0,,,,Sociology,big data,workshop,10
92fd5aaeacaa332a725e72647e20baec5c73b73d,Big Data Analytics in Supply Chain Management: A Systematic Literature Review and Research Directions,2022.0,"Big data analytics has been successfully used for various business functions, such as accounting, marketing, supply chain, and operations. Currently, along with the recent development in machine learning and computing infrastructure, big data analytics in the supply chain are surging in importance. In light of the great interest and evolving nature of big data analytics in supply chains, this study conducts a systematic review of existing studies in big data analytics. This study presents a framework of a systematic literature review from interdisciplinary perspectives. From the organizational perspective, this study examines the theoretical foundations and research models that explain the sustainability and performances achieved through the use of big data analytics. Then, from the technical perspective, this study analyzes types of big data analytics, techniques, algorithms, and features developed for enhanced supply chain functions. Finally, this study identifies the research gap and suggests future research directions.",10.3390/bdcc6010017,Big Data and Cognitive Computing,Computer Science,big data,workshop,10
b904dcdbd7c7b33938583f2f57d05ca70e121ea9,An Efficient and Secure Big Data Storage in Cloud Environment by Using Triple Data Encryption Standard,2022.0,"In recent decades, big data analysis has become the most important research topic. Hence, big data security offers Cloud application security and monitoring to host highly sensitive data to support Cloud platforms. However, the privacy and security of big data has become an emerging issue that restricts the organization to utilize Cloud services. The existing privacy preserving approaches showed several drawbacks such as a lack of data privacy and accurate data analysis, a lack of efficiency of performance, and completely rely on third party. In order to overcome such an issue, the Triple Data Encryption Standard (TDES) methodology is proposed to provide security for big data in the Cloud environment. The proposed TDES methodology provides a relatively simpler technique by increasing the sizes of keys in Data Encryption Standard (DES) to protect against attacks and defend the privacy of data. The experimental results showed that the proposed TDES method is effective in providing security and privacy to big healthcare data in the Cloud environment. The proposed TDES methodology showed less encryption and decryption time compared to the existing Intelligent Framework for Healthcare Data Security (IFHDS) method.",10.3390/bdcc6040101,Big Data and Cognitive Computing,Computer Science,big data,workshop,15
1597449a7f64b6bd24639b4deab96c8a8c184177,"Digital twin-driven product design, manufacturing and service with big data",2017.0,,10.1007/s00170-017-0233-1,The International Journal of Advanced Manufacturing Technology,Engineering,big data,journal,7
f12930cd5f58990badc1a7c5d2749cad004cfb0e,Big data analytics for intelligent manufacturing systems: A review,2021.0,,10.1016/J.JMSY.2021.03.005,,Computer Science,big data,workshop,13
fe44200fed05f9a7c656f2245deded8fd5f5e1e6,CatBoost for big data: an interdisciplinary review,2020.0,"Gradient Boosted Decision Trees (GBDT’s) are a powerful tool for classification and regression tasks in Big Data. Researchers should be familiar with the strengths and weaknesses of current implementations of GBDT’s in order to use them effectively and make successful contributions. CatBoost is a member of the family of GBDT machine learning ensemble techniques. Since its debut in late 2018, researchers have successfully used CatBoost for machine learning studies involving Big Data. We take this opportunity to review recent research on CatBoost as it relates to Big Data, and learn best practices from studies that cast CatBoost in a positive light, as well as studies where CatBoost does not outshine other techniques, since we can learn lessons from both types of scenarios. Furthermore, as a Decision Tree based algorithm, CatBoost is well-suited to machine learning tasks involving categorical, heterogeneous data. Recent work across multiple disciplines illustrates CatBoost’s effectiveness and shortcomings in classification and regression tasks. Another important issue we expose in literature on CatBoost is its sensitivity to hyper-parameters and the importance of hyper-parameter tuning. One contribution we make is to take an interdisciplinary approach to cover studies related to CatBoost in a single work. This provides researchers an in-depth understanding to help clarify proper application of CatBoost in solving problems. To the best of our knowledge, this is the first survey that studies all works related to CatBoost in a single publication.",10.1186/s40537-020-00369-8,Journal of Big Data,"Computer Science,Medicine",big data,journal,8
3962b74e02f2bcdb634d51bf51e5765807006a44,"MetaboAnalyst 6.0: towards a unified platform for metabolomics data processing, analysis and interpretation",2024.0,"Abstract We introduce MetaboAnalyst version 6.0 as a unified platform for processing, analyzing, and interpreting data from targeted as well as untargeted metabolomics studies using liquid chromatography - mass spectrometry (LC–MS). The two main objectives in developing version 6.0 are to support tandem MS (MS2) data processing and annotation, as well as to support the analysis of data from exposomics studies and related experiments. Key features of MetaboAnalyst 6.0 include: (i) a significantly enhanced Spectra Processing module with support for MS2 data and the asari algorithm; (ii) a MS2 Peak Annotation module based on comprehensive MS2 reference databases with fragment-level annotation; (iii) a new Statistical Analysis module dedicated for handling complex study design with multiple factors or phenotypic descriptors; (iv) a Causal Analysis module for estimating metabolite - phenotype causal relations based on two-sample Mendelian randomization, and (v) a Dose-Response Analysis module for benchmark dose calculations. In addition, we have also improved MetaboAnalyst's visualization functions, updated its compound database and metabolite sets, and significantly expanded its pathway analysis support to around 130 species. MetaboAnalyst 6.0 is freely available at https://www.metaboanalyst.ca.",10.1093/nar/gkae253,Nucleic Acids Research,Medicine,data processing,workshop,11
047eb33ad77581b3bb9611450a917ac2e6967d66,Revolutionizing Supply Chain Management: Real-time Data Processing and Concurrency,2024.0,"In the contemporary business landscape, effective supply chain management (SCM) is paramount for organizations seeking to thrive amidst evolving market dynamics and heightened customer expectations. This research paper presents a pioneering approach to SCM that harnesses cutting-edge technologies, namely Kafka and Akka, to revolutionize data integration and decision-making processes. By leveraging Kafka as a robust distributed event streaming platform and Akka as a versatile toolkit for developing concurrent and distributed applications, our system facilitates seamless communication and coordination across diverse nodes within the supply chain network. This paper elucidates the intricacies of the proposed architecture, detailing the implementation methodology and performance evaluation metrics. Through a comprehensive examination, we demonstrate how our solution enhances supply chain visibility, fosters operational agility, and enables real-time responsiveness to market fluctuations and customer demands. Moreover, practical use cases exemplify the transformative impact of our approach on inventory management optimization, order fulfillment efficiency, and logistics optimization. Furthermore, we delve into the challenges encountered during implementation and deployment, offering insights into potential mitigative strategies. Finally, we outline avenues for future research, exploring emerging trends and opportunities in the realm of SCM empowered by Kafka and Akka technologies.",10.38124/ijisrt/ijisrt24may207,International Journal of Innovative Science and Research Technology,,data processing,journal,5
627be67feb084f1266cfc36e5aed3c3e7e6ce5f0,MapReduce: simplified data processing on large clusters,2008.0,"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.",10.1145/1327452.1327492,CACM,Computer Science,data processing,journal,11
b204970b0503a923359bff532726666f5e0e971b,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,2012.0,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",10.1093/nar/gks1219,Nucleic Acids Res.,"Computer Science,Medicine,Biology",data processing,workshop,10
48265726215736f7dd7ceccacac488422032397c,DPABI: Data Processing & Analysis for (Resting-State) Brain Imaging,2016.0,,10.1007/s12021-016-9299-4,Neuroinformatics,"Medicine,Computer Science",data processing,workshop,5
02b1607af35b48f0bd716367caf6a7428b969369,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,2019.0,"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",,International Conference on Learning Representations,"Mathematics,Computer Science",data processing,conference,14
2880ac931c11176aee6d42a7e7bb0703aacde3f9,Open3D: A Modern Library for 3D Data Processing,2018.0,"Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.",,arXiv.org,Computer Science,data processing,workshop,14
bbed457fd04ba4972018382d1a01a0bdde399d3c,Proteome Discoverer—A Community Enhanced Data Processing Suite for Protein Informatics,2021.0,"Proteomics researchers today face an interesting challenge: how to choose among the dozens of data processing and analysis pipelines available for converting tandem mass spectrometry files to protein identifications. Due to the dominance of Orbitrap technology in proteomics in recent history, many researchers have defaulted to the vendor software Proteome Discoverer. Over the fourteen years since the initial release of the software, it has evolved in parallel with the increasingly complex demands faced by proteomics researchers. Today, Proteome Discoverer exists in two distinct forms with both powerful commercial versions and fully functional free versions in use in many labs today. Throughout the 11 main versions released to date, a central theme of the software has always been the ability to easily view and verify the spectra from which identifications are made. This ability is, even today, a key differentiator from other data analysis solutions. In this review I will attempt to summarize the history and evolution of Proteome Discoverer from its first launch to the versions in use today.",10.3390/proteomes9010015,Proteomes,Medicine,data processing,workshop,5
39602922b04885047254444fd1a1586d797617ce,Optimization of metabolomic data processing using NOREVA,2021.0,,10.1038/s41596-021-00636-9,Nature Protocols,Medicine,data processing,journal,8
41d04aa3c25dcfbf1b44ce666c48759e03c216c7,tf.data: A Machine Learning Data Processing Framework,2021.0,"Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators which can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions allow users to focus on the application logic of data processing, while tf.data's runtime ensures that pipelines run efficiently.  We demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Google's fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer.",10.14778/3476311.3476374,Proceedings of the VLDB Endowment,Computer Science,data processing,workshop,14
c15f30a3e84910a28cc560e7db097fd99339e8c1,HiC-Pro: an optimized and flexible pipeline for Hi-C data processing,2015.0,"HiC-Pro is an optimized and flexible pipeline for processing Hi-C data from raw reads to normalized contact maps. HiC-Pro maps reads, detects valid ligation products, performs quality controls and generates intra- and inter-chromosomal contact maps. It includes a fast implementation of the iterative correction method and is based on a memory-efficient data format for Hi-C contact maps. In addition, HiC-Pro can use phased genotype data to build allele-specific contact maps. We applied HiC-Pro to different Hi-C datasets, demonstrating its ability to easily process large data in a reasonable time. Source code and documentation are available at http://github.com/nservant/HiC-Pro.",10.1186/s13059-015-0831-x,Genome Biology,"Biology,Medicine",data processing,workshop,5
57a3fc6d0aaad3c10c793b4e59390ca04c935282,"An Overview of IoT Sensor Data Processing, Fusion, and Analysis Techniques",2020.0,"In the recent era of the Internet of Things, the dominant role of sensors and the Internet provides a solution to a wide variety of real-life problems. Such applications include smart city, smart healthcare systems, smart building, smart transport and smart environment. However, the real-time IoT sensor data include several challenges, such as a deluge of unclean sensor data and a high resource-consumption cost. As such, this paper addresses how to process IoT sensor data, fusion with other data sources, and analyses to produce knowledgeable insight into hidden data patterns for rapid decision-making. This paper addresses the data processing techniques such as data denoising, data outlier detection, missing data imputation and data aggregation. Further, it elaborates on the necessity of data fusion and various data fusion methods such as direct fusion, associated feature extraction, and identity declaration data fusion. This paper also aims to address data analysis integration with emerging technologies, such as cloud computing, fog computing and edge computing, towards various challenges in IoT sensor network and sensor data analysis. In summary, this paper is the first of its kind to present a complete overview of IoT sensor data processing, fusion and analysis techniques.",10.3390/s20216076,Italian National Conference on Sensors,"Computer Science,Medicine",data processing,conference,15
c756be2ce7de11627511d068383b62f6ab85d5c4,Data Processing,2018.0,,10.1007/978-1-4939-7131-2_100251,Encyclopedia of Social Network Analysis and Mining. 2nd Ed.,Computer Science,data processing,workshop,14
ada0b87cd5c30d31186c38fb12e631d29426a3bf,Spark SQL: Relational Data Processing in Spark,2015.0,"Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.",10.1145/2723372.2742797,SIGMOD Conference,Computer Science,data processing,conference,15
ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee,Learning scheduling algorithms for data processing clusters,2018.0,"Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems use simple, generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective, such as minimizing average job completion time. However, off-the-shelf RL techniques cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima improves average job completion time by at least 21% over hand-tuned scheduling heuristics, achieving up to 2x improvement during periods of high cluster load.",10.1145/3341302.3342080,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication","Computer Science,Mathematics",data processing,conference,15
5938eb834f05b4ae64dcdabfaa6d4bef70eb0825,First M87 Event Horizon Telescope Results. III. Data Processing and Calibration,2019.0,"We present the calibration and reduction of Event Horizon Telescope (EHT) 1.3 mm radio wavelength observations of the supermassive black hole candidate at the center of the radio galaxy M87 and the quasar 3C 279, taken during the 2017 April 5–11 observing campaign. These global very long baseline interferometric observations include for the first time the highly sensitive Atacama Large Millimeter/submillimeter Array (ALMA); reaching an angular resolution of 25 μas, with characteristic sensitivity limits of ~1 mJy on baselines to ALMA and ~10 mJy on other baselines. The observations present challenges for existing data processing tools, arising from the rapid atmospheric phase fluctuations, wide recording bandwidth, and highly heterogeneous array. In response, we developed three independent pipelines for phase calibration and fringe detection, each tailored to the specific needs of the EHT. The final data products include calibrated total intensity amplitude and phase information. They are validated through a series of quality assurance tests that show consistency across pipelines and set limits on baseline systematic errors of 2% in amplitude and 1° in phase. The M87 data reveal the presence of two nulls in correlated flux density at ~3.4 and ~8.3 Gλ and temporal evolution in closure quantities, indicating intrinsic variability of compact structure on a timescale of days, or several light-crossing times for a few billion solar-mass black hole. These measurements provide the first opportunity to image horizon-scale structure in M87.",10.3847/2041-8213/ab0c57,Astrophysical Journal,Physics,data processing,journal,13
1a60a9d1eef24e123c27a9eee5a399ac2b620fee,FedLoc: Federated Learning Framework for Data-Driven Cooperative Localization and Location Data Processing,2020.0,"In this overview paper, data-driven learning model-based cooperative localization and location data processing are considered, in line with the emerging machine learning and big data methods. We first review (1) state-of-the-art algorithms in the context of federated learning, (2) two widely used learning models, namely the deep neural network model and the Gaussian process model, and (3) various distributed model hyper-parameter optimization schemes. Then, we demonstrate various practical use cases that are summarized from a mixture of standard, newly published, and unpublished works, which cover a broad range of location services, including collaborative static localization/fingerprinting, indoor target tracking, outdoor navigation using low-sampling GPS, and spatio-temporal wireless traffic data modeling and prediction. Experimental results show that near centralized data fitting- and prediction performance can be achieved by a set of collaborative mobile users running distributed algorithms. All the surveyed use cases fall under our newly proposed Federated Localization (FedLoc) framework, which targets on collaboratively building accurate location services without sacrificing user privacy, in particular, sensitive information related to their geographical trajectories. Future research directions are also discussed at the end of this paper.",10.1109/OJSP.2020.3036276,IEEE Open Journal of Signal Processing,Computer Science,data processing,journal,15
febe776e285dc5e72c7e3ee697a87a794e1c00ff,Privacy-Preserving Data Processing with Flexible Access Control,2020.0,"Cloud computing provides an efficient and convenient platform for cloud users to store, process and control their data. Cloud overcomes the bottlenecks of resource-constrained user devices and greatly releases their storage and computing burdens. However, due to the lack of full trust in cloud service providers, the cloud users generally prefer to outsource their sensitive data in an encrypted form, which, however, seriously complicates data processing, analysis, as well as access control. Homomorphic encryption (HE) as a single key system cannot flexibly control data sharing and access after encrypted data processing. How to realize various computations over encrypted data in an efficient way and at the same time flexibly control the access to data processing results has been an important challenging issue. In this paper, we propose a privacy-preserving data processing scheme with flexible access control. With the cooperation of a data service provider (DSP) and a computation party (CP), our scheme, based on Paillier's partial homomorphic encryption (PHE), realizes seven basic operations, i.e., Addition, Subtraction, Multiplication, Sign Acquisition, Absolute, Comparison, and Equality Test, over outsourced encrypted data. In addition, our scheme, based on the homomorphism of attribute-based encryption (ABE), is also designed to support flexible access control over processing results of encrypted data. We further prove the security of our scheme and demonstrate its efficiency and advantages through simulations and comparisons with existing work.",10.1109/TDSC.2017.2786247,IEEE Transactions on Dependable and Secure Computing,Computer Science,data processing,journal,12
915ab7b6ca3633230403d47dbb31cf74888cc5c9,A Survey on Automatic Parameter Tuning for Big Data Processing Systems,2020.0,"Big data processing systems (e.g., Hadoop, Spark, Storm) contain a vast number of configuration parameters controlling parallelism, I/O behavior, memory settings, and compression. Improper parameter settings can cause significant performance degradation and stability issues. However, regular users and even expert administrators grapple with understanding and tuning them to achieve good performance. We investigate existing approaches on parameter tuning for both batch and stream data processing systems and classify them into six categories: rule-based, cost modeling, simulation-based, experiment-driven, machine learning, and adaptive tuning. We summarize the pros and cons of each approach and raise some open research problems for automatic parameter tuning.",10.1145/3381027,ACM Computing Surveys,Computer Science,data processing,journal,6
409896dee6e4e3de5fe0d62c3d8b78498d36229b,DolphinNext: a distributed data processing platform for high throughput genomics,2020.0,"The emergence of high throughput technologies that produce vast amounts of genomic data, such as next-generation sequencing (NGS) is transforming biological research. The dramatic increase in the volume of data, the variety and continuous change of data processing tools, algorithms and databases make analysis the main bottleneck for scientific discovery. The processing of high throughput datasets typically involves many different computational programs, each of which performs a specific step in a pipeline. Given the wide range of applications and organizational infrastructures, there is a great need for highly parallel, flexible, portable, and reproducible data processing frameworks. Several platforms currently exist for the design and execution of complex pipelines. Unfortunately, current platforms lack the necessary combination of parallelism, portability, flexibility and/or reproducibility that are required by the current research environment. To address these shortcomings, workflow frameworks that provide a platform to develop and share portable pipelines have recently arisen. We complement these new platforms by providing a graphical user interface to create, maintain, and execute complex pipelines. Such a platform will simplify robust and reproducible workflow creation for non-technical users as well as provide a robust platform to maintain pipelines for large organizations. To simplify development, maintenance, and execution of complex pipelines we created DolphinNext. DolphinNext facilitates building and deployment of complex pipelines using a modular approach implemented in a graphical interface that relies on the powerful Nextflow workflow framework by providing 1. A drag and drop user interface that visualizes pipelines and allows users to create pipelines without familiarity in underlying programming languages. 2. Modules to execute and monitor pipelines in distributed computing environments such as high-performance clusters and/or cloud 3. Reproducible pipelines with version tracking and stand-alone versions that can be run independently. 4. Modular process design with process revisioning support to increase reusability and pipeline development efficiency. 5. Pipeline sharing with GitHub and automated testing 6. Extensive reports with R-markdown and shiny support for interactive data visualization and analysis. DolphinNext is a flexible, intuitive, web-based data processing and analysis platform that enables creating, deploying, sharing, and executing complex Nextflow pipelines with extensive revisioning and interactive reporting to enhance reproducible results.",10.1186/s12864-020-6714-x,BMC Genomics,"Medicine,Biology",data processing,workshop,7
66781e11f7de7302cac2ec6fd6a0d9f77030687e,3 Dimensional analysis of holographic photopolymers based memories,2005.0,,,,,data storage,workshop,15
10ef65b38efce54e0d6b575a895859ebd4ab678c,Data Storage Using DNA,2023.0,"The exponential growth of global data has outpaced the storage capacities of current technologies, necessitating innovative storage strategies. DNA, as a natural medium for preserving genetic information, has emerged as a highly promising candidate for next‐generation storage medium. Storing data in DNA offers several advantages, including ultrahigh physical density and exceptional durability. Facilitated by significant advancements in various technologies, such as DNA synthesis, DNA sequencing, and DNA nanotechnology, remarkable progress has been made in the field of DNA data storage over the past decade. However, several challenges still need to be addressed to realize practical applications of DNA data storage. In this review, the processes and strategies of in vitro DNA data storage are first introduced, highlighting recent advancements. Next, a brief overview of in vivo DNA data storage is provided, with a focus on the various writing strategies developed to date. At last, the challenges encountered in each step of DNA data storage are summarized and promising techniques are discussed that hold great promise in overcoming these obstacles.",10.1002/adma.202307499,Advances in Materials,Medicine,data storage,workshop,12
182acc7a1c9a63cfdef9f86ee9df43fd75e05060,"Memristive technologies for data storage, computation, encryption, and radio-frequency communication",2022.0,"Memristive devices, which combine a resistor with memory functions such that voltage pulses can change their resistance (and hence their memory state) in a nonvolatile manner, are beginning to be implemented in integrated circuits for memory applications. However, memristive devices could have applications in many other technologies, such as non–von Neumann in-memory computing in crossbar arrays, random number generation for data security, and radio-frequency switches for mobile communications. Progress toward the integration of memristive devices in commercial solid-state electronic circuits and other potential applications will depend on performance and reliability challenges that still need to be addressed, as described here. Description Putting memristors to work Memristors, which are resistors that change conductivity and act as memories, are not only being used in commercial computing but have several application areas in computing and communications. Lanza et al. review how devices such as phase-change memories, resistive random-access memories, and magnetoresistive random-access memories are being integrated into silicon electronics. Memristors also are finding use in artificial intelligence when integrated in three-dimensional crossbar arrays for low-power, non–von Neuman architectures. Other applications include random-number generation for data encryption and radiofrequency switches for mobile communications. —PDS A review explains how resistors with memory functions are being integrated into electronics and new computer architectures. BACKGROUND Memristive devices exhibit an electrical resistance that can be adjusted to two or more nonvolatile levels by applying electrical stresses. The core of the most advanced memristive devices is a metal/insulator/metal nanocell made of phase-change, metal-oxide, magnetic, or ferroelectric materials, which is often placed in series with other circuit elements (resistor, selector, transistor) to enhance their performance in array configurations (i.e., avoid damage during state transition, minimize intercell disturbance). The memristive effect was discovered in 1969 and the first commercial product appeared in 2006, consisting of a 4-megabit nonvolatile memory based on magnetic materials. In the past few years, the switching endurance, data retention time, energy consumption, switching time, integration density, and price of memristive nonvolatile memories has been remarkably improved (depending on the materials used, values up to ~1015 cycles, >10 years, ~0.1 pJ, ~10 ns, 256 gigabits per die, and ≤$0.30 per gigabit have been achieved). ADVANCES As of 2021, memristive memories are being used as standalone memory and are also embedded in application-specific integrated circuits for the Internet of Things (smart watches and glasses, medical equipment, computers), and their market value exceeds $621 million. Recent studies have shown that memristive devices may also be exploited for advanced computation, data security, and mobile communication. Advanced computation refers to the hardware implementation of artificial neural networks by exploiting memristive attributes such as progressive conductance increase and decrease, vector matrix multiplication (in crossbar arrays), and spike timing–dependent plasticity; state-of-the-art developments have achieved >10 trillion operations per second per watt. Data encryption can be realized by exploiting the stochasticity inherent in the memristive effect, which manifests as random fluctuations (within a given range) of the switching voltages/times and state currents. For example, true random number generator and physical unclonable functions produce random codes when exposing a population of memristive devices to an electrical stress at 50% of switching probability (it is impossible to predict which devices will switch because that depends on their atomic structure). Mobile communication can also benefit from memristive devices because they could be employed as 5G and terahertz switches with low energy consumption owing to the nonvolatile nature of the resistive states; the current commercial technology is based on silicon transistors, but they are volatile and consume data both during switching and when idle. State-of-the-art developments have achieved cutoff frequencies of >100 THz with excellent insertion loss and isolation. OUTLOOK Consolidating memristive memories in the market and creating new commercial memristive technologies requires further enhancement of their performance, integration density, and cost, which may be achieved via materials and structure engineering. Market forecasts expect the memristive memories market to grow up to ~$5.6 billion by 2026, which will represent ~2% of the nearly $280 billion memory market. Phase-change and metal-oxide memristive memories should improve switching endurance and reduce energy consumption and variability, and the magnetic ones should offer improved integration density. Ferroelectric memristive memories still suffer low switching endurance, which is hindering commercialization. The figures of merit of memristive devices for advanced computation highly depend on the application, but maximizing endurance, retention, and conductance range while minimizing temporal conductance fluctuations are general goals. Memristive devices for data encryption and mobile communication require higher switching endurance, and two-dimensional materials prototypes are being investigated. Part of Science’s coverage of the 75th anniversary of the discovery of the transistor Fundamental memristive effects and their applications. Memristive devices, in which electrical resistance can be adjusted to two or more nonvolatile levels, can be fabricated using different materials (top row). This allows adjusting their performance to fulfill the requirements of different technologies. Memristive memories are a reality, and important progress is being achieved in advanced computation, security systems, and mobile communication (bottom row).",10.1126/science.abj9979,Science,Medicine,data storage,workshop,13
7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91,Emerging Approaches to DNA Data Storage: Challenges and Prospects,2022.0,"With the total amount of worldwide data skyrocketing, the global data storage demand is predicted to grow to 1.75 × 1014 GB by 2025. Traditional storage methods have difficulties keeping pace given that current storage media have a maximum density of 103 GB/mm3. As such, data production will far exceed the capacity of currently available storage methods. The costs of maintaining and transferring data, as well as the limited lifespans and significant data losses associated with current technologies also demand advanced solutions for information storage. Nature offers a powerful alternative through the storage of information that defines living organisms in unique orders of four bases (A, T, C, G) located in molecules called deoxyribonucleic acid (DNA). DNA molecules as information carriers have many advantages over traditional storage media. Their high storage density, potentially low maintenance cost, ease of synthesis, and chemical modification make them an ideal alternative for information storage. To this end, rapid progress has been made over the past decade by exploiting user-defined DNA materials to encode information. In this review, we discuss the most recent advances of DNA-based data storage with a major focus on the challenges that remain in this promising field, including the current intrinsic low speed in data writing and reading and the high cost per byte stored. Alternatively, data storage relying on DNA nanostructures (as opposed to DNA sequence) as well as on other combinations of nanomaterials and biomolecules are proposed with promising technological and economic advantages. In summarizing the advances that have been made and underlining the challenges that remain, we provide a roadmap for the ongoing research in this rapidly growing field, which will enable the development of technological solutions to the global demand for superior storage methodologies.",10.1021/acsnano.2c06748,ACS Nano,Medicine,data storage,workshop,5
86dbd884043eb5807c61d2c65b813e673b4a04fa,Blockchain-Based Secure Data Storage Protocol for Sensors in the Industrial Internet of Things,2022.0,"The Industrial Internet of Things (IIoT) that introduces Internet of Things (IoT) technology into industrial environments is beneficial to construct smart factories. It utilizes various sensors to collect the data of industrial devices. These data are analyzed to improve the manufacturing efficiency and product quality. Cloud storage provides a solution for storing data outsourced, especially for sensors that have limited local storage and computational capacity. To ensure the privacy preserving of devices, the collected data should be stored in the formal ciphertext. Therefore, encrypted data sharing should be implemented to analyze the devices’ data. In this article, the cloud storage solution for sensors is considered. To achieve a secure and efficient data storage and sharing, a novel group signature scheme, which has less computation overhead and communication overhead, is designed to realize anonymous authentication first. And then, a novel blockchain-based cloud storage protocol for sensors in IIoT is constructed on basis of the proposed group signature scheme. Smart contract and proxy re-encryption are utilized in this protocol to realize secure data sharing with a less computational overhead. Furthermore, security proofs and performance evaluations demonstrate that this protocol is secure, privacy-preserving, and has at least 40% and 20% performance improvement in data storage and sharing phase, respectively.",10.1109/TII.2021.3112601,IEEE Transactions on Industrial Informatics,Computer Science,data storage,journal,12
24ab4e99e582c9770281eee0a39cbeb70ddd891a,Information-Theoretic Foundations of DNA Data Storage,2022.0,"Due to its longevity and enormous information density, DNA is an attractive medium for archival data storage. Natural DNA more than 700.000 years old has been recovered, and about 5 grams of DNA can in principle hold a Zetabyte of digital information, orders of magnitude more than what is achieved on conventional storage media. Thanks to rapid technological advances, DNA storage is becoming practically feasible, as demonstrated by a number of experimental storage systems, making it a promising solution for our society’s increasing need of data storage. While in living things, DNA molecules can consist of millions of nucleotides, due to technological constraints, in practice, data is stored on many short DNA molecules, which are preserved in a DNA pool and cannot be spatially ordered. Moreover, imperfections in sequencing, synthesis, and handling, as well as DNA decay during storage, introduce random noise into the system, making the task of reliably storing and retrieving information in DNA challenging. This unique setup raises a natural information-theoretic question: how much information can be reliably stored on and reconstructed from millions of short noisy sequences? The goal of this monograph is to address this question by discussing the fundamental limits of storing information on DNA. Motivated by current technological constraints on DNA synthesis and sequencing, we propose a probabilistic channel model that captures three key distinctive aspects of the DNA storage systems: (1) the data is written onto many short DNA molecules that are stored in an unordered fashion; (2) the molecules are corrupted by noise and (3) the data is read by randomly sampling from the DNA pool. Our goal is to investigate the impact of each of these key aspects on the capacity of the DNA storage system. Rather than focusing on coding-theoretic considerations and computationally efficient encoding and decoding, we aim to build an information-theoretic foundation for the analysis of these channels, developing tools for achievability and converse arguments. This is a preprint of the following publication: Ilan Shomorony and Reinhard Heckel (2022),“Information-Theoretic Foundations of DNA Data Storage”, Foundations and Trends in Communications and Information Theory: Vol. 19, No. 1, pp 1-106. DOI: 10.1561/0100000117. ar X iv :2 21 1. 05 55 2v 1 [ cs .I T ] 1 0 N ov 2 02 2",10.1561/0100000117,Foundations and Trends in Communications and Information Theory,"Computer Science,Mathematics",data storage,workshop,5
b904dcdbd7c7b33938583f2f57d05ca70e121ea9,An Efficient and Secure Big Data Storage in Cloud Environment by Using Triple Data Encryption Standard,2022.0,"In recent decades, big data analysis has become the most important research topic. Hence, big data security offers Cloud application security and monitoring to host highly sensitive data to support Cloud platforms. However, the privacy and security of big data has become an emerging issue that restricts the organization to utilize Cloud services. The existing privacy preserving approaches showed several drawbacks such as a lack of data privacy and accurate data analysis, a lack of efficiency of performance, and completely rely on third party. In order to overcome such an issue, the Triple Data Encryption Standard (TDES) methodology is proposed to provide security for big data in the Cloud environment. The proposed TDES methodology provides a relatively simpler technique by increasing the sizes of keys in Data Encryption Standard (DES) to protect against attacks and defend the privacy of data. The experimental results showed that the proposed TDES method is effective in providing security and privacy to big healthcare data in the Cloud environment. The proposed TDES methodology showed less encryption and decryption time compared to the existing Intelligent Framework for Healthcare Data Security (IFHDS) method.",10.3390/bdcc6040101,Big Data and Cognitive Computing,Computer Science,data storage,workshop,7
eccad1576e6c67b2c93a5cb8d384d038fbb161d6,DNA stability: a central design consideration for DNA data storage systems,2021.0,"Data storage in DNA is a rapidly evolving technology that could be a transformative solution for the rising energy, materials, and space needs of modern information storage. Given that the information medium is DNA itself, its stability under different storage and processing conditions will fundamentally impact and constrain design considerations and data system capabilities. Here we analyze the storage conditions, molecular mechanisms, and stabilization strategies influencing DNA stability and pose specific design configurations and scenarios for future systems that best leverage the considerable advantages of DNA storage. DNA has the potential to store vast amounts of data but it is subject to physical decay. In this Perspective, the authors propose that the stability of DNA should be a key consideration in how it is used for data storage.",10.1038/s41467-021-21587-5,Nature Communications,"Computer Science,Medicine",data storage,journal,14
1e1cf81a1113482be3f0c280db994a832cb9426a,Secure Data Storage and Recovery in Industrial Blockchain Network Environments,2020.0,"The massive redundant data storage and communication in network 4.0 environments have issues of low integrity, high cost, and easy tampering. To address these issues, in this article, a secure data storage and recovery scheme in the blockchain-based network is proposed by improving the decentration, tampering-proof, real-time monitoring, and management of storage systems, as such design supports the dynamic storage, fast repair, and update of distributed data in the data storage system of industrial nodes. A local regenerative code technology is used to repair and store data between failed nodes while ensuring the privacy of user data. That is, as the data stored are found to be damaged, multiple local repair groups constructed by vector code can simultaneously yet efficiently repair multiple distributed data storage nodes. Based on the unique chain storage structure, such as data consensus mechanism and smart contract, the storage structure of blockchain distributed coding not only quickly repair the nearby local regenerative codes in the blockchain but also reduce the resource overhead in the data storage process of industrial nodes. Experimental results show that the proposed scheme improves the repair rate of multinode data by 9% and data storage rate increased by 8.6%, indicating to be promising with good security and real-time performance.",10.1109/TII.2020.2966069,IEEE Transactions on Industrial Informatics,Computer Science,data storage,journal,14
7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616,Random access in large-scale DNA data storage,2018.0,,10.1038/nbt.4079,Nature Biotechnology,"Computer Science,Medicine",data storage,journal,13
287a7da1801a07cf7fd85ffcc23c79504876ecc0,An artificial chromosome for data storage,2021.0,"Abstract DNA digital storage provides an alternative for information storage with high density and long-term stability. Here, we report the de novo design and synthesis of an artificial chromosome that encodes two pictures and a video clip. The encoding paradigm utilizing the superposition of sparsified error correction codewords and pseudo-random sequences tolerates base insertions/deletions and is well suited to error-prone nanopore sequencing for data retrieval. The entire 254 kb sequence was 95.27% occupied by encoded data. The Transformation-Associated Recombination method was used in the construction of this chromosome from DNA fragments and necessary autonomous replication sequences. The stability was demonstrated by transmitting the data-carrying chromosome to the 100th generation. This study demonstrates a data storage method using encoded artificial chromosomes via in vivo assembly for write-once and stable replication for multiple retrievals, similar to a compact disc, with potential in economically massive data distribution.",10.1093/nsr/nwab028,National Science Review,"Medicine,Biology",data storage,workshop,5
c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1,Molecular digital data storage using DNA,2019.0,,10.1038/s41576-019-0125-3,Nature reviews genetics,"Medicine,Biology",data storage,workshop,12
b80e2af7ffa85fe2f02ea8390e4915d55c19d199,Advances in Emerging Memory Technologies: From Data Storage to Artificial Intelligence,2021.0,"This paper presents an overview of emerging memory technologies. It begins with the presentation of stand-alone and embedded memory technology evolution, since the appearance of Flash memory in the 1980s. Then, the progress of emerging memory technologies (based on filamentary, phase change, magnetic, and ferroelectric mechanisms) is presented with a review of the major demonstrations in the literature. The potential of these technologies for storage applications addressing various markets and products is discussed. Finally, we discuss how the rise of artificial intelligence and bio-inspired circuits offers an opportunity for emerging memory technology and shifts the application from pure data storage to storage and computing tasks, and also enlarges the range of required specifications at the device level due to the exponential number of new systems and architectures.",10.3390/app112311254,Applied Sciences,,data storage,workshop,10
3f43bcb910df8c1a76de79057a63195e6c6bc258,Spintronic devices for energy-efficient data storage and energy harvesting,2020.0,"The current data revolution has, in part, been enabled by decades of research into magnetism and spin phenomena. For example, milestones such as the observation of giant magnetoresistance, and the resulting development of the spin-valve read head, continue to motivate device research. However, the ever-growing need for higher data processing speeds and larger data storage capabilities has caused a significant increase in energy consumption and environmental concerns. Ongoing research and development in spintronics should therefore reduce energy consumption while increasing information processing capabilities. Here, we provide an overview of the current status of research and technology developments in data storage and spin-mediated energy harvesting in relation to energy-efficient technologies. We give our perspective on the advantages and outstanding issues for various data-storage concepts, and energy conversion mechanisms enabled by spin. The current surge in data generation necessitates devices that can store and analyze data in an energy efficient way. This Review summarizes and discusses developments on the use of spintronic devices for energy-efficient data storage and logic applications, and energy harvesting based on spin.",10.1038/s43246-020-0022-5,Communications Materials,Computer Science,data storage,workshop,12
18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,2020.0,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",10.1109/TII.2019.2916689,IEEE Transactions on Industrial Informatics,Computer Science,data storage,journal,5
10d89b13a6309a531c35701d37d3bd76a27a3942,Big Data Storage,2021.0,"This chapter provides an overview of big data storage technologies. It is the result of a survey of the current state of the art in data storage technologies in order to create a cross-sectorial technology roadmap. This chapter provides a concise overview of big data storage systems that are capable of dealing with high velocity, high volumes, and high varieties of data. It describes distributed file systems, NoSQL databases, graph databases, and NewSQL databases. The chapter investigates the challenge of storing data in a secure and privacy-preserving way. The social and economic impact of big data storage technologies is described, open research challenges highlighted, and three selected case studies are provided from the health, finance, and energy sector. Some of the key insights on big data storage are (1) in-memory databases and columnar databases typically outperform traditional relational database systems, (2) the major technical barrier to widespread up-take of big data storage solutions are missing standards, and (3) there is a need to address open research challenges related to the scalability and performance of graph databases.",10.1007/978-3-319-21569-3_7,New Horizons for a Data-Driven Economy,Computer Science,data storage,workshop,13
6544259ff6b335b1dcec75e031b6d57e5b9509f4,Scaling DNA data storage with nanoscale electrode wells,2021.0,A demonstration of DNA synthesis control at over 1000x higher density shows the path to large scale DNA data storage.,10.1126/sciadv.abi6714,Science Advances,Medicine,data storage,workshop,15
18fff2d1ef494f2726b93d2f8a119b874f2e3d1d,Low cost DNA data storage using photolithographic synthesis and advanced information reconstruction and error correction,2020.0,"Due to its longevity and enormous information density, DNA is an attractive medium for archival storage. The current hamstring of DNA data storage systems—both in cost and speed—is synthesis. The key idea for breaking this bottleneck pursued in this work is to move beyond the low-error and expensive synthesis employed almost exclusively in today’s systems, towards cheaper, potentially faster, but high-error synthesis technologies. Here, we demonstrate a DNA storage system that relies on massively parallel light-directed synthesis, which is considerably cheaper than conventional solid-phase synthesis. However, this technology has a high sequence error rate when optimized for speed. We demonstrate that even in this high-error regime, reliable storage of information is possible, by developing a pipeline of algorithms for encoding and reconstruction of the information. In our experiments, we store a file containing sheet music of Mozart, and show perfect data recovery from low synthesis fidelity DNA. The current bottleneck for DNA data storage systems is the cost and speed of synthesis. Here, the authors use inexpensive, massively parallel light-directed synthesis and correct for a high error rate with a pipeline of encoding and reconstruction algorithms.",10.1038/s41467-020-19148-3,Nature Communications,"Computer Science,Medicine",data storage,journal,15
fc61c7221350806c25379f385c27b2102ff8eb57,Uncertainties in synthetic DNA-based data storage,2021.0,"Abstract Deoxyribonucleic acid (DNA) has evolved to be a naturally selected, robust biomacromolecule for gene information storage, and biological evolution and various diseases can find their origin in uncertainties in DNA-related processes (e.g. replication and expression). Recently, synthetic DNA has emerged as a compelling molecular media for digital data storage, and it is superior to the conventional electronic memory devices in theoretical retention time, power consumption, storage density, and so forth. However, uncertainties in the in vitro DNA synthesis and sequencing, along with its conjugation chemistry and preservation conditions can lead to severe errors and data loss, which limit its practical application. To maintain data integrity, complicated error correction algorithms and substantial data redundancy are usually required, which can significantly limit the efficiency and scale-up of the technology. Herein, we summarize the general procedures of the state-of-the-art DNA-based digital data storage methods (e.g. write, read, and preservation), highlighting the uncertainties involved in each step as well as potential approaches to correct them. We also discuss challenges yet to overcome and research trends in the promising field of DNA-based data storage.",10.1093/nar/gkab230,Nucleic Acids Research,Medicine,data storage,workshop,11
11be2469ab1d1c508e7b6e14148990741ba87884,Nanopore-Based DNA Hard Drives for Rewritable and Secure Data Storage.,2020.0,"Nanopores are powerful single-molecule tools for label-free sensing of nanoscale molecules including DNA, which can be used for building designed nanostructures and performing computations. Here, DNA hard drives (DNA-HDs) are introduced based on DNA nanotechnology and nanopore sensing as a rewritable molecular memory system, allowing for storing, operating and reading data in the changeable three-dimensional structure of DNA. Writing and erasing data are significantly improved compared to previous molecular storage systems by employing controllable attachment and removal of molecules on a long double-stranded DNA. Data reading is achieved by detecting the single molecules at the microsecond timescale using nanopores. The DNA-HD also ensures secure data storage where the data can only be read after providing the correct physical molecular keys. Our approach allows for easy-writing and easy-reading, rewritable and secure data storage toward a promising miniature scale integration for molecular data storage and computation.",10.1021/acs.nanolett.0c00755,Nano letters (Print),"Materials Science,Medicine",data storage,journal,9
2d57f5be0f829cdd3633ba28433bd2e3149db8e5,Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey,2023.0,"The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.",10.1109/TKDE.2024.3400824,IEEE Transactions on Knowledge and Data Engineering,Computer Science,data querying,journal,11
714f47bbedcadd7ebc44d2d5010f13323fc6a256,GraphQL-LD: Linked Data Querying with GraphQL,2018.0,,,International Workshop on the Semantic Web,Computer Science,data querying,workshop,11
4419c5720e30d5ca5158795d4c848125650b8db1,Opportunistic Linked Data Querying Through Approximate Membership Metadata,2015.0,,10.1007/978-3-319-25007-6_6,International Workshop on the Semantic Web,Computer Science,data querying,workshop,9
398af3c05b9d3e194d419c3af049a76fb4e3afbb,Defining Energy Consumption Plans for Data Querying Processes,2014.0,"During the last few years, we have been witnessing a significant increase in research about the development and production of hardware and software components with low levels of energy consumption. Today, energy consumption is one of the most critical issues in the area of information technologies and communication. One of the fractions in which this concern is most evident is in the management of database systems, with particular emphasis on those commonly designated as data centers. On these systems daily run a large amount of data querying processes, monitored and controlled by high sophisticated database management systems, which are responsible to establish efficient processing plans to support them. Using the information provided by a querying execution plan, especially the one related to the operators they used to perform database operations, we designed and developed an alternative method to define energy consumption plans for database queries. In this paper we present how such method works on the estimation of the energy consumption of each database operator integrated in the execution plan of a query at compile time. With it, we build up its corresponding energy consumption plan for executing the query, taking into consideration as well the characteristics of the computational platforms used for that.",10.1109/BDCloud.2014.109,2014 IEEE Fourth International Conference on Big Data and Cloud Computing,Computer Science,data querying,conference,9
d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b,BimSPARQL: Domain-specific functional SPARQL extensions for querying RDF building data,2018.0,"In this paper, we propose to extend SPARQL functions for querying Industry Foundation Classes (IFC) building data. The official IFC documentation and BIM requirement checking use cases are used to drive the development of the proposed functionality. By extending these functions, we aim to (1) simplify writing queries and (2) retrieve useful information implied in 3D geometry data according to requirement checking use cases. Extended functions are modelled as RDF vocabularies and classified into groups for further extensions. We combine declarative rules with procedural programming to implement extended functions. Realistic requirement checking scenarios are used to evaluate and demonstrate the effectiveness of this approach and indicate query performance. Compared with query techniques developed in the conventional Building Information Modeling domain, we show the added value of such approach by providing an application example of querying building and regulatory data, where spatial and logic reasoning can be applied and data from multiple sources are required. Based on the implementation and evaluation work, we discuss the advantages and applicability of this approach, current issues and future challenges.",10.3233/SW-180297,Semantic Web,Computer Science,data querying,workshop,14
1fa4936fb06319c3f4536c26a447d5507c92bd48,A Privacy-Preserving and Verifiable Querying Scheme in Vehicular Fog Data Dissemination,2019.0,"Vehicular fog has attracted considerable attention recently, as the densely deployed fog devices are in proximity to vehicular end-users, and they are particularly suitable for the latency-sensitive and location-aware vehicular services. In this paper, we propose a secure querying scheme in vehicular fog data dissemination, in which the roadside units (RSUs) act as fog storage devices to cache data at network edge and disseminate data upon querying. To disrupt the association between a specific data request and its origin vehicle, the proposed scheme exploits an invertible matrix to structure multiple data requests from different vehicles, and aggregates the ciphertexts of data requests at the RSU side with the homomorphic Paillier cryptosystem. Meanwhile, given the invertible matrix and decryption result, the RSU can recover each individual data request without identifying its origin vehicle. In addition, the RSU can verify the correctness of the recovered data requests with an identity-based batch verification scheme. Through security analysis, we demonstrate that the proposed scheme can achieve the security goals of unlinkability, confidentiality, and verifiability. Performance evaluations are also conducted, in which the obtained results show that the proposed scheme can be adaptive to the fluctuating number of the data querying vehicles, and significantly reduce the computation complexity and communication overhead.",10.1109/TVT.2018.2888854,IEEE Transactions on Vehicular Technology,Computer Science,data querying,journal,10
4bedaac1acb45ec28d06db7cf49f9b77faf9f652,Private-HERMES: a benchmark framework for privacy-preserving mobility data querying and mining methods,2012.0,"Mobility data sources feed larger and larger trajectory databases nowadays. Due to the need of extracting useful knowledge patterns that improve services based on users' and customers' behavior, querying and mining such databases has gained significant attention in recent years. However, publishing mobility data may lead to severe privacy violations. In this paper, we present Private-HERMES, an integrated platform for applying data mining and privacy-preserving querying over mobility data. The presented platform provides a two-dimension benchmark framework that includes: (i) a query engine that provides privacy-aware data management functionality of the in-house data via a set of auditing mechanisms that protect the sensitive information against several types of attacks, and (ii) a progressive analysis framework, which, apart from anonymization methods for data publishing, includes various well-known mobility data mining techniques to evaluate the effect of anonymization in the querying and mining results. The demonstration of Private-HERMES via a real-world case study, illustrates the flexibility and usefulness of the platform for supporting privacy-aware data analysis, as well as for providing an extensible blueprint benchmark architecture for privacy-preservation related methods in mobility data.",10.1145/2247596.2247675,International Conference on Extending Database Technology,Computer Science,data querying,conference,13
babc3d8ddb6436511cc013cb3bbd57a59f3e872a,Improving the Recall of Live Linked Data Querying through Reasoning,2012.0,,10.1007/978-3-642-33203-6_14,International Conference on Web Reasoning and Rule Systems,Computer Science,data querying,conference,14
ac67d5f9c89d8d72fbd074f94079608220348f3f,ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores,2016.0,"In this paper, we present ATHENA, an ontology-driven system for natural language querying of complex relational databases. Natural language interfaces to databases enable users easy access to data, without the need to learn a complex query language, such as SQL. ATHENA uses domain specific ontologies, which describe the semantic entities, and their relationships in a domain. We propose a unique two-stage approach, where the input natural language query (NLQ) is first translated into an intermediate query language over the ontology, called OQL, and subsequently translated into SQL. Our two-stage approach allows us to decouple the physical layout of the data in the relational store from the semantics of the query, providing physical independence. Moreover, ontologies provide richer semantic information, such as inheritance and membership relations, that are lost in a relational schema. By reasoning over the ontologies, our NLQ engine is able to accurately capture the user intent. We study the effectiveness of our approach using three different workloads on top of geographical (GEO), academic (MAS) and financial (FIN) data. ATHENA achieves 100% precision on the GEO and MAS workloads, and 99% precision on the FIN workload which operates on a complex financial ontology. Moreover, ATHENA attains 87.2%, 88.3%, and 88.9% recall on the GEO, MAS, and FIN workloads, respectively.",10.14778/2994509.2994536,Proceedings of the VLDB Endowment,Computer Science,data querying,workshop,5
fa77a44f3f1857361a50c3137d623c35ef8a5739,Querying Graphs with Data,2016.0,"Graph databases have received much attention as of late due to numerous applications in which data is naturally viewed as a graph; these include social networks, RDF and the Semantic Web, biological databases, and many others. There are many proposals for query languages for graph databases that mainly fall into two categories. One views graphs as a particular kind of relational data and uses traditional relational mechanisms for querying. The other concentrates on querying the topology of the graph. These approaches, however, lack the ability to combine data and topology, which would allow queries asking how data changes along paths and patterns enveloping it. In this article, we present a comprehensive study of languages that enable such combination of data and topology querying. These languages come in two flavors. The first follows the standard approach of path queries, which specify how labels of edges change along a path, but now we extend them with ways of specifying how both labels and data change. From the complexity point of view, the right type of formalisms are subclasses of register automata. These, however, are not well suited for querying. To overcome this, we develop several types of extended regular expressions to specify paths with data and study their querying power and complexity. The second approach adopts the popular XML language XPath and extends it from XML documents to graphs. Depending on the exact set of allowed features, we have a family of languages, and our study shows that it includes efficient and highly expressive formalisms for querying both the structure of the data and the data itself.",10.1145/2850413,Journal of the ACM,Computer Science,data querying,journal,15
5b34752817bc0d6aa96466dabcbc24a83dd071fe,SPARQLByE: Querying RDF data by example,2016.0,"Semantic Web technologies such as RDF and its query language, SPARQL, offer the possibility of opening up the use of public datasets to a great variety of ordinary users. But a key obstacle to the use of open data is the unfamiliarity of users with the structure of data or with SPARQL. To deal with these issues, we introduce a system for querying RDF data by example. At its core is a technique for reverse-engineering SPARQL queries by example. We demonstrate how reverse engineering along with other techniques, such as query relaxation, enables our system, SPARQLByE, to guide users who are unfamiliar with both the dataset and with SPARQL to the desired query and result set.",10.14778/3007263.3007302,Proceedings of the VLDB Endowment,Computer Science,data querying,workshop,9
fd0496ab020acf366375615ab40235e6dd3c5897,Querying Geo-Textual Data: Spatial Keyword Queries and Beyond,2016.0,"Over the past decade, we have moved from a predominantly desktop based web to a predominantly mobile web, where users most often access the web from mobile devices such as smartphones. In addition, we are witnessing a proliferation of geo-located, textual web content. Motivated in part by these developments, the research community has been hard at work enabling the efficient computation of a variety of query functionality on geo-textual data, yielding a sizable body of literature on the querying of geo-textual data. With a focus on different types of keyword-based queries on geo-textual data, the tutorial also explores topics such as continuous queries on streaming geo-textual data, queries that retrieve attractive regions of geo-textual objects, and queries that extract properties, e.g., topics and top-$k$ frequent words, of the objects in regions. The tutorial is designed to offer an overview of the problems addressed in this body of literature and offers an overview of pertinent concepts and techniques. In addition, the tutorial suggests open problems and new research direction.",10.1145/2882903.2912572,SIGMOD Conference,Computer Science,data querying,conference,10
c7660220afe8c91b726a012ceac194f4940bd893,Informating HRM: a comparison of data querying and data mining,2010.0,"Beyond mere automation of tasks, a major potential of Human Resource Information Systems (HRIS) is to informate Human Resource Management (HRM). Within current HRIS, the informate function is realised based on a data querying approach. Given a major innovation in data analysis subsumed under the concept of 'data mining', possibly valuable potentials to informate HRM are lost while overlooking the data mining approach. Therefore our paper aims at a conceptual evaluation of both approaches. We therefore discuss and evaluate data mining as a novel approach compared to data querying as the conventional approach to informating HRM. Based on a robust framework of informational contributions, our analysis reveals interesting potentials of data mining to generate explicative and prognostic information. Thus data mining enriches and complements the conventional querying approach. Furthermore, recommendations for future research are derived in order to deepen the knowledge on the contributions of data mining to informate HRM.",10.1504/IJBIS.2010.030629,International Journal of Business Information Systems,Computer Science,data querying,journal,14
7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2,"Storing, Tracking, and Querying Provenance in Linked Data",2017.0,"The proliferation of heterogeneous Linked Data on the Web poses new challenges to database systems. In particular, the capacity to store, track, and query provenance data is becoming a pivotal feature of modern triplestores. We present methods extending a native RDF store to efficiently handle the storage, tracking, and querying of provenance in RDF data. We describe a reliable and understandable specification of the way results were derived from the data and how particular pieces of data were combined to answer a query. Subsequently, we present techniques to tailor queries with provenance data. We empirically evaluate the presented methods and show that the overhead of storing and tracking provenance is acceptable. Finally, we show that tailoring a query with provenance information can also significantly improve the performance of query execution.",10.1109/TKDE.2017.2690299,IEEE Transactions on Knowledge and Data Engineering,Computer Science,data querying,journal,5
8332d7dfa45120e880acf81694204ece421e8270,Querying and mining of time series data: experimental comparison of representations and distance measures,2008.0,"The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic.",10.14778/1454159.1454226,Proceedings of the VLDB Endowment,Computer Science,data querying,workshop,12
e4788ee4f5e90c6f42cedc5116acd2d6475c3180,QUIS: InSitu Heterogeneous Data Source Querying,2017.0,"Existing data integration frameworks are poorly suited for the special requirements of scientists. To answer a specific research question, often, excerpts of data from different sources need to be integrated. The relevant parts and the set of underlying sources may differ from query to query. The analyses also oftentimes involve frequently changing data and exploratory querying. Additionally, The data sources not only store data in different formats, but also provide inconsistent data access functionality. The classic Extract-Transform-Load (ETL) approach seems too complex and time-consuming and does not fit well with interest and expertise of the scientists.    With QUIS (QUery In-Situ), we provide a solution for this problem. QUIS is an open source heterogeneous in-situ data querying system. It utilizes a federated query virtualization approach that is built upon plugged-in adapters. QUIS takes a user query and transforms appropriate portions of it into the corresponding computation model on individual data sources and executes it. It complements the segments of the query that the target data sources can not execute. Hence, it guarantees full syntax and semantic support for its language on all data sources. QUIS's in-situ querying facility almost eliminates the time to prepare the data while maintaining a competitive performance and steady scalability.    The present demonstration illustrates interesting features of the system: virtual Schemas, heterogeneous joins, and visual query results. We provide a realistic data processing scenario to examine the system's features. Users can interact with QUIS using its desktop workbench, command line interface, or from any R client including RStudio Server.",10.14778/3137765.3137798,Proceedings of the VLDB Endowment,Computer Science,data querying,workshop,10
256ccb81d8c3bb63f4299195584b8e9f9d60187a,CloudMdsQL: querying heterogeneous cloud data stores with a common language,2015.0,,10.1007/s10619-015-7185-y,Distributed and parallel databases,Computer Science,data querying,workshop,5
19b93280f17696a4ddfa2c75490a50ab107addf2,SHAHED: A MapReduce-based system for querying and visualizing spatio-temporal satellite data,2015.0,"Remote sensing data collected by satellites are now made publicly available by several space agencies. This data is very useful for scientists pursuing research in several applications including climate change, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations for natural phenomena such as temperature and vegetation. Unfortunately, the use of such data is very limited due to the huge size of archives (> 500TB) and the limited capabilities of traditional applications. This paper introduces SHAHED; a MapReduce-based system for querying, visualizing, and mining large scale satellite data. SHAHED considers both the spatial and temporal aspects of the data to provide efficient query processing at large scale. The core of SHAHED is composed of four main components. The uncertainty component recovers missing data in the input which comes from cloud coverage and satellite mis-alignment. The indexing component provides a novel multi-resolution quad-tree-based spatio-temporal index structure, which indexes satellite data efficiently with minimal space overhead. The querying component answers selection and aggregate queries in real-time using the constructed index. Finally, the visualization component uses MapReduce programs to generate heat map images and videos for user queries. A set of experiments running on a live system deployed on a cluster of machines show the efficiency of the proposed design. All the features supported by SHAHED are made accessible through an easy to use Web interface that hides the complexity of the system and provides a nice user experience.",10.1109/ICDE.2015.7113427,IEEE International Conference on Data Engineering,Computer Science,data querying,conference,13
1d2d3fa967d7688e352a86ed77ff762fc6c555ec,Fuzzy functional dependency and its application to approximate data querying,2000.0,"Reviews a new definition of fuzzy functional dependency based on conditional probability and its application to approximate data reduction related to the operation of projection in classical relational databases in order to construct fuzzy integrity constraints. We introduce the concept of partial fuzzy functional dependency, which expresses the fact that a given attribute X does not determine Y completely, but in the partial area of X it might determine Y. Finally, we discuss another application of fuzzy functional dependency in constructing fuzzy query relations for data querying and approximate joins of two or more fuzzy query relations in the framework of an extended query system.",10.1109/IDEAS.2000.880561,Proceedings - International Database Engineering and Applications Symposium,Computer Science,data querying,conference,13
52e510271b172d098ec9b107a4159216ec08527e,Querying Big Data by Accessing Small Data,2015.0,"This paper investigates the feasibility of querying big data by accessing a bounded amount of the data. We study boundedly evaluable queries under a form of access constraints, when their evaluation cost is determined by the queries and constraints only. While it is undecidable to determine whether FO queries are boundedly evaluable, we show that for several classes of FO queries, the bounded evaluability problem is decidable. We also provide characterization and effective syntax for their boundedly evaluable queries. When a query Q is not boundedly evaluable, we study two approaches to approximately answering Q under access constraints. (1) We search for upper and lower envelopes of Q that are boundedly evaluable and warrant a constant accuracy bound. (2) We instantiate a minimum set of variables (parameters) in Q such that the specialized query is boundedly evaluable. We study problems for deciding the existence of envelopes and bounded specialized queries, and establish their complexity for various classes of FO queries.",10.1145/2745754.2745771,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,"Mathematics,Computer Science",data querying,conference,5
57e7a7323f58a35f5e2cc33bf17d4ac9cdcafdd4,Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources,2008.0,,10.1038/nprot.2008.211,Nature Protocols,"Medicine,Biology",bioinformatics,journal,11
f76fb3641b7fdbb8aba2eb40217d0c17dcdca682,BIOINFORMATICS APPLICATIONS NOTE,2001.0,,,,,bioinformatics,workshop,15
fa60b6806050255a77699bd0f9f5d824884c5162,Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists,2008.0,"Functional analysis of large gene lists, derived in most cases from emerging high-throughput genomic, proteomic and bioinformatics scanning approaches, is still a challenging and daunting task. The gene-annotation enrichment analysis is a promising high-throughput strategy that increases the likelihood for investigators to identify biological processes most pertinent to their study. Approximately 68 bioinformatics enrichment tools that are currently available in the community are collected in this survey. Tools are uniquely categorized into three major classes, according to their underlying enrichment algorithms. The comprehensive collections, unique tool classifications and associated questions/issues will provide a more comprehensive and up-to-date view regarding the advantages, pitfalls and recent trends in a simpler tool-class level rather than by a tool-by-tool approach. Thus, the survey will help tool designers/developers and experienced end users understand the underlying algorithms and pertinent details of particular tool categories/tools, enabling them to make the best choices for their particular research interests.",10.1093/nar/gkn923,Nucleic Acids Research,"Computer Science,Medicine,Biology",bioinformatics,workshop,5
fd495d6cf7c3169bc58550fdf32be6e16e2800f8,Bioconductor: open software development for computational biology and bioinformatics,2004.0,"The Bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. The goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. We describe details of our aims and methods, identify current challenges, compare Bioconductor to other open bioinformatics projects, and provide working examples.",10.1186/gb-2004-5-10-r80,Genome Biology,"Medicine,Biology",bioinformatics,workshop,14
90485e0ce54c1ad12a2d01362a007ab107d71063,Biopython: freely available Python tools for computational molecular biology and bioinformatics,2009.0,"Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning. Availability: Biopython is freely available, with documentation and source code at www.biopython.org under the Biopython license. Contact: All queries should be directed to the Biopython mailing lists, see www.biopython.org/wiki/_Mailing_listspeter.cock@scri.ac.uk.",10.1093/bioinformatics/btp163,Bioinform.,"Medicine,Computer Science",bioinformatics,workshop,5
63c4adf203e2e0f9895ff82485a0a701cf3cb650,The nf-core framework for community-curated bioinformatics pipelines,2020.0,,10.1038/s41587-020-0439-x,Nature Biotechnology,"Medicine,Computer Science",bioinformatics,journal,15
2fabff697cd0ec343490f89e8e6587200d97f378,"Sangerbox: A comprehensive, interaction‐friendly clinical bioinformatics analysis platform",2022.0,"Abstract In recent decades, with the continuous development of high‐throughput sequencing technology, data volume in medical research has increased, at the same time, almost all clinical researchers have their own independent omics data, which provided a better condition for data mining and a deeper understanding of gene functions. However, for these large amounts of data, many common and cutting‐edge effective bioinformatics research methods still cannot be widely used. This has encouraged the establishment of many analytical platforms, a portion of databases or platforms were designed to solve the special analysis needs of users, for instance, MG RAST, IMG/M, Qiita, BIGSdb, and TRAPR were developed for specific omics research, and some databases or servers provide solutions for special problems solutions. Metascape was designed to only provide functional annotations of genes as well as function enrichment analysis; BioNumerics and RidomSeqSphere+ perform multilocus sequence typing; CARD provides only antimicrobial resistance annotations. Additionally, some web services are outdated, and inefficient interaction often fails to meet the needs of researchers, such as our previous versions of the platform. Therefore, the demand to complete massive data processing tasks urgently requires a comprehensive bioinformatics analysis platform. Hence, we have developed a website platform, Sangerbox 3.0 (http://vip.sangerbox.com/), a web‐based tool platform. On a user‐friendly interface that also supports differential analysis, the platform provides interactive customizable analysis tools, including various kinds of correlation analyses, pathway enrichment analysis, weighted correlation network analysis, and other common tools and functions, users only need to upload their own corresponding data into Sangerbox 3.0, select required parameters, submit, and wait for the results after the task has been completed. We have also established a new interactive plotting system that allows users to adjust the parameters in the image; moreover, optimized plotting performance enables users to adjust large‐capacity vector maps on the web site. At the same time, we have integrated GEO, TCGA, ICGC, and other databases and processed data in batches, greatly reducing the difficulty to obtain data and improving the efficiency of bioimformatics study for users. Finally, we also provide users with rich sources of bioinformatics analysis courses, offering a platform for researchers to share and exchange knowledge.",10.1002/imt2.36,iMeta,Medicine,bioinformatics,workshop,12
98c25683fc8d6446448b734b1bcf08e1457f8d85,A review of feature selection techniques in bioinformatics,2007.0,"Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.",10.1093/bioinformatics/btm344,Bioinform.,"Computer Science,Medicine",bioinformatics,workshop,13
7b1a7f8fcab2d1671cd907c9bafa81cb784bac1c,"Introducing the Bacterial and Viral Bioinformatics Resource Center (BV-BRC): a resource combining PATRIC, IRD and ViPR",2022.0,"The National Institute of Allergy and Infectious Diseases (NIAID) established the Bioinformatics Resource Center (BRC) program to assist researchers with analyzing the growing body of genome sequence and other omics-related data. In this report, we describe the merger of the PAThosystems Resource Integration Center (PATRIC), the Influenza Research Database (IRD) and the Virus Pathogen Database and Analysis Resource (ViPR) BRCs to form the Bacterial and Viral Bioinformatics Resource Center (BV-BRC) https://www.bv-brc.org/. The combined BV-BRC leverages the functionality of the bacterial and viral resources to provide a unified data model, enhanced web-based visualization and analysis tools, bioinformatics services, and a powerful suite of command line tools that benefit the bacterial and viral research communities.",10.1093/nar/gkac1003,Nucleic Acids Res.,"Medicine,Computer Science",bioinformatics,workshop,14
eb41f630c7fbd1b41a4fbb41dc4b1bffa8e1cf95,BIOINFORMATICS ORIGINAL PAPER,,,,,,bioinformatics,workshop,10
1ff4bd599b950218f0517fb76ee49ad0599e1c53,A Completely Reimplemented MPI Bioinformatics Toolkit with a New HHpred Server at its Core.,2017.0,,10.1016/j.jmb.2017.12.007,Journal of Molecular Biology,"Computer Science,Medicine",bioinformatics,journal,12
625bdb87e5750faf96fd0c59cb274ff9dda9cbb3,Snakemake - a scalable bioinformatics workflow engine,2018.0,,10.1093/bioinformatics/bty350,Bioinform.,"Computer Science,Medicine",bioinformatics,workshop,5
a41d8c4eddf4054ef080c7edec21b39c492892ee,"Nanopore sequencing technology, bioinformatics and applications",2021.0,,10.1038/s41587-021-01108-x,Nature Biotechnology,Medicine,bioinformatics,journal,15
7389b6ebbf36f4d869a02e305e2ef52ad2c92264,Applications of transformer-based language models in bioinformatics: a survey,2023.0,"Abstract Summary The transformer-based language models, including vanilla transformer, BERT and GPT-3, have achieved revolutionary breakthroughs in the field of natural language processing (NLP). Since there are inherent similarities between various biological sequences and natural languages, the remarkable interpretability and adaptability of these models have prompted a new wave of their application in bioinformatics research. To provide a timely and comprehensive review, we introduce key developments of transformer-based language models by describing the detailed structure of transformers and summarize their contribution to a wide range of bioinformatics research from basic sequence analysis to drug discovery. While transformer-based applications in bioinformatics are diverse and multifaceted, we identify and discuss the common challenges, including heterogeneity of training data, computational expense and model interpretability, and opportunities in the context of bioinformatics research. We hope that the broader community of NLP researchers, bioinformaticians and biologists will be brought together to foster future research and development in transformer-based language models, and inspire novel bioinformatics applications that are unattainable by traditional methods. Supplementary information Supplementary data are available at Bioinformatics Advances online.",10.1093/bioadv/vbad001,Bioinformatics Advances,Medicine,bioinformatics,workshop,9
70d692ea21491d6d776b2509026e2d366a315fff,BIOINFORMATICS APPLICATIONS NOTE,,,,,,bioinformatics,workshop,12
be620beb5e8209fefd28cf26b288c74c7ae30db1,Diffusion models in bioinformatics and computational biology.,2023.0,,10.1038/s44222-023-00114-9,Nature Reviews Bioengineering,Medicine,bioinformatics,workshop,7
1b54ecd153bcb9283bacf12d448a12ff871b51fc,BIOINFORMATICS APPLICATIONS NOTE Structural bioinformatics,2005.0,,,,,bioinformatics,workshop,13
0ff76dd78e47f4534ee148b644f1f2707bc70df5,"Improvements to PATRIC, the all-bacterial Bioinformatics Database and Analysis Resource Center",2016.0,"The Pathosystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center (https://www.patricbrc.org). Recent changes to PATRIC include a redesign of the web interface and some new services that provide users with a platform that takes them from raw reads to an integrated analysis experience. The redesigned interface allows researchers direct access to tools and data, and the emphasis has changed to user-created genome-groups, with detailed summaries and views of the data that researchers have selected. Perhaps the biggest change has been the enhanced capability for researchers to analyze their private data and compare it to the available public data. Researchers can assemble their raw sequence reads and annotate the contigs using RASTtk. PATRIC also provides services for RNA-Seq, variation, model reconstruction and differential expression analysis, all delivered through an updated private workspace. Private data can be compared by ‘virtual integration’ to any of PATRIC's public data. The number of genomes available for comparison in PATRIC has expanded to over 80 000, with a special emphasis on genomes with antimicrobial resistance data. PATRIC uses this data to improve both subsystem annotation and k-mer classification, and tags new genomes as having signatures that indicate susceptibility or resistance to specific antibiotics.",10.1093/nar/gkw1017,Nucleic Acids Res.,"Computer Science,Medicine,Biology",bioinformatics,workshop,13
703a72ad206272d2022c9b1d7eb775e275f4b39c,Empowering Beginners in Bioinformatics with ChatGPT,2023.0,"The impressive conversational and programming abilities of ChatGPT make it an attractive tool for facilitating the education of bioinformatics data analysis for beginners. In this study, we proposed an iterative model to fine-tune instructions for guiding a ChatGPT in generating code for bioinformatics data analysis tasks. We demonstrated the feasibility of the model by applying it to various bioinformatics topics. Additionally, we discussed practical considerations and limitations regarding the use of the model in chatbot-aided bioinformatics education.",10.1101/2023.03.07.531414,bioRxiv,"Medicine,Computer Science,Biology",bioinformatics,workshop,12
8002fefd7bcc66c23a8d49aa9a7ae8d4a9885ad3,"Expasy, the Swiss Bioinformatics Resource Portal, as designed by its users",2021.0,"Abstract The SIB Swiss Institute of Bioinformatics (https://www.sib.swiss) creates, maintains and disseminates a portfolio of reliable and state-of-the-art bioinformatics services and resources for the storage, analysis and interpretation of biological data. Through Expasy (https://www.expasy.org), the Swiss Bioinformatics Resource Portal, the scientific community worldwide, freely accesses more than 160 SIB resources supporting a wide range of life science and biomedical research areas. In 2020, Expasy was redesigned through a user-centric approach, known as User-Centred Design (UCD), whose aim is to create user interfaces that are easy-to-use, efficient and targeting the intended community. This approach, widely used in other fields such as marketing, e-commerce, and design of mobile applications, is still scarcely explored in bioinformatics. In total, around 50 people were actively involved, including internal stakeholders and end-users. In addition to an optimised interface that meets users' needs and expectations, the new version of Expasy provides an up-to-date and accurate description of high-quality resources based on a standardised ontology, allowing to connect functionally-related resources.",10.1093/nar/gkab225,Nucleic Acids Res.,"Medicine,Computer Science",bioinformatics,workshop,8
